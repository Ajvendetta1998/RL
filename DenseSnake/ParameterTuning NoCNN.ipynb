{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alhus\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import numpy as np \n",
    "import os \n",
    "import sys \n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Move the model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQL:\n",
    "    def __init__(self, model, criterion, optimizer, actions, discount_factor=0.6, exploration_rate=0.9, memory_size=100000, batch_size=500,base_decay_rate = 0.995, decay_rate=0.98565207, base_exploration_rate = 0.1,validation_batch_size = 100 ,epochs = 1,device = 'cpu'):\n",
    "        #NN\n",
    "        self.model = model\n",
    "        self.actions = actions\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        #gamma\n",
    "        self.discount_factor = discount_factor\n",
    "        #for epsilon-greedy\n",
    "        self.exploration_rate = exploration_rate\n",
    "        #buffer\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.evalmemory  = deque(maxlen = memory_size)\n",
    "        self.batch_size = batch_size\n",
    "        #diminish exploration \n",
    "        self.base_decay_rate = base_decay_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.validation_batch_size = validation_batch_size\n",
    "        self.base_exploration_rate = base_exploration_rate\n",
    "\n",
    "    def get_action(self, state, direction, snake_list, block_size, width, height):\n",
    "        action = direction\n",
    "        possible_moves = list(range(0, len(self.actions)))\n",
    "\n",
    "        # eliminate all impossible moves\n",
    "        acts = list(self.actions)\n",
    "        poss_copy = possible_moves.copy()\n",
    "        for p in poss_copy:\n",
    "            (u, v) = (snake_list[-1][0] + acts[p][1] * block_size, snake_list[-1][1] + acts[p][0] * block_size)\n",
    "            if (u < 0 or u >= width or v < 0 or v >= height or [u, v] in snake_list[1:]):\n",
    "                possible_moves.remove(p)\n",
    "\n",
    "        if len(possible_moves) > 0:\n",
    "            if np.random.rand() < self.base_exploration_rate + self.exploration_rate:\n",
    "                # Choose a random action\n",
    "                action = possible_moves[np.random.randint(len(possible_moves))]\n",
    "            else:\n",
    "                # Choose the best action according to the model\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    q_values = self.model(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device))\n",
    "                    q_values = q_values.squeeze(0)\n",
    "\n",
    " \n",
    "                q_sorted = np.array([q_values.cpu()[i] for i in possible_moves])\n",
    "                #action = possible_moves[q_sorted.argmax()]\n",
    "                gamma = 5\n",
    "                soft_maxed = np.exp(gamma *q_sorted)/sum(np.exp(gamma * q_sorted))\n",
    "\n",
    "                action = possible_moves[np.random.choice(len(soft_maxed),p = soft_maxed)]\n",
    "        return action\n",
    "\n",
    "\n",
    "    def add_memory(self, state, action, reward, next_state, done,episode_reward):\n",
    "        x = np.random.rand()\n",
    "\n",
    "        if(x<0.9):\n",
    "            self.memory.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.evalmemory.append((state, action, reward, next_state, done))\n",
    "        episode_reward=episode_reward*self.discount_factor+reward\n",
    "        return episode_reward\n",
    "    \n",
    "    def train(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            # Not enough memories to train the model\n",
    "            return\n",
    "\n",
    "        # Randomly sample memories from the replay buffer\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "\n",
    "        states = torch.tensor(states,dtype = torch.float32).to(self.device)\n",
    "\n",
    "\n",
    "        next_states = torch.tensor(next_states,dtype = torch.float32).to(self.device)\n",
    "\n",
    "\n",
    "        # Decrease the exploration rate\n",
    "        self.exploration_rate *= self.decay_rate\n",
    "        self.base_exploration_rate *= self.base_decay_rate\n",
    "\n",
    "        target_q_values = np.zeros((batch_size,len(self.actions)))\n",
    "\n",
    "        # Calculate the target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.model(next_states).to(self.device)\n",
    "            for i in range(batch_size):\n",
    "                if dones[i]:\n",
    "                    target_q_values[i][actions[i]] = rewards[i]\n",
    "\n",
    "                else:\n",
    "                    target_q_values[i][actions[i]] = rewards[i] + self.discount_factor * max(next_q_values[i])\n",
    "        target_q_values = torch.tensor(target_q_values,dtype = torch.float32).to(self.device)\n",
    "\n",
    "        # Train the model for a specified number of epochs\n",
    "        for _ in range(self.epochs):\n",
    "            predicted_q_values = self.model(states).float().to(self.device)\n",
    "            loss = nn.functional.mse_loss(predicted_q_values, target_q_values)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Remove variables from memory\n",
    "        del states, actions, rewards, next_states, dones, target_q_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Snake block size\n",
    "block_size = 25\n",
    "\n",
    "\n",
    "# Set display width and height\n",
    "width = 500 \n",
    "height = 500\n",
    "penalty_names  = ['accessible_points_proportion','penalty_distance','penalty_touch_self','penalty_distance*gass_reward','reward_eat','penalty_wall','penalty_danger','compacity','episode_len_penalty']\n",
    "c = np.array([0.4,0.5,0.3,0.1,0.6,0.3,0.2,0.1,0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_food(snake_list):\n",
    "    # Generate food for the snake where there is no snake\n",
    "    food_x, food_y = None, None\n",
    "    \n",
    "    while food_x is None or food_y is None or [food_x, food_y] in snake_list:\n",
    "        food_x = round(random.randrange(0, width - block_size) / block_size) * block_size\n",
    "        food_y = round(random.randrange(0, height - block_size) / block_size) * block_size\n",
    "    return food_x, food_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary of possible actions \n",
    "actions = {\"up\":(-1,0),\"down\":(1,0),\"left\":(0,-1),\"right\":(0,1)}\n",
    "\n",
    "#(x,y) apple + snake body (which has at most width*height parts) \n",
    "input_size = 2*width*height//block_size**2+2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initNNmodel():\n",
    "    # Define the input shape\n",
    "    input_shape = (3, height//block_size, width//block_size)\n",
    "\n",
    "    # Create a Sequential model\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_size, 4096),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4096, 2048),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2048, 1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(1024, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, len(actions))\n",
    "    )\n",
    "\n",
    "    # Send the model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Print the model summary\n",
    "    print(model)\n",
    "\n",
    "    return model, criterion, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state(snake_list,apple):\n",
    "    s = np.array(snake_list)\n",
    "\n",
    "\n",
    "    input = np.zeros(input_size)\n",
    "    input[0],input[1] = apple[0]/width,apple[1]/height\n",
    "    for u in range(len(snake_list)):\n",
    "        if(2*u+2>=input_size):\n",
    "            break\n",
    "        input[2*u+2],input[2*u+3]= s[len(snake_list)-1-u][0]/width,s[len(snake_list)-1-u][1]/height\n",
    "    return input\n",
    "\n",
    "def normalized_distance(u,v,food_x,food_y):\n",
    "    return np.sqrt((((u-food_x)/width)**2+((v-food_y)/height)**2)/2)\n",
    "\n",
    "def inBounds(u,v):\n",
    "    if(u>=0 and v>=0):\n",
    "        if(u<width and v<height):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def gaussian_aroundone(x,alpha):\n",
    "    return(np.exp(-alpha*(x-1)**2))\n",
    "def danger_distance(direction, snake_list):\n",
    "    dis =0  \n",
    "    acts = list(actions.values())\n",
    "    (u,v) = (snake_list[-1][0],snake_list[-1][1])\n",
    "    while (inBounds(u,v)):\n",
    "        u +=block_size*acts[direction][1]\n",
    "        v+=block_size*acts[direction][0]\n",
    "        if([u,v] in snake_list[1:]):\n",
    "            return (-1+1.0*dis*block_size/max(width,height))\n",
    "        dis+=1\n",
    "    return(0)\n",
    "\n",
    "#reward function for each state and action\n",
    "def reward(action, snake_list,episode_length,c):\n",
    "    copy = deepcopy(snake_list)\n",
    "    p = copy[-1]\n",
    "    a = list(actions.values())\n",
    "    (u,v)=(a[action][1]*block_size+p[0],a[action][0]*block_size+p[1])\n",
    "    penalty_touch_self = 0 \n",
    "    if [u,v] in snake_list:\n",
    "        penalty_touch_self=-1 # return a negative reward if the snake collides with itself\n",
    "    copy.append([u,v])\n",
    "    del copy[0]\n",
    "    \n",
    "    global food_x, food_y\n",
    "    # reward the agent for getting closer to the food\n",
    "    reward_distance = 1-normalized_distance(u,v,food_x,food_y)\n",
    "    #if too far then the reward is very close to 0 \n",
    "    gass_reward =gaussian_aroundone(reward_distance,20)\n",
    "    # reward the agent for eating the food\n",
    "    reward_eat = 1 if u == food_x and v == food_y else 0\n",
    "    # penalize the agent for moving away from the food\n",
    "    penalty_distance = -1 if normalized_distance(u,v,food_x,food_y) > normalized_distance(p[0], p[1], food_x, food_y) else 0.5\n",
    "    # penalize he agent for hitting a wall\n",
    "    penalty_wall = -1 if not (inBounds(u,v)) else 0\n",
    "    #penalize the agent for getting closer to danger\n",
    "    penalty_danger = danger_distance(action,snake_list)\n",
    "    #print(penalty_danger)\n",
    "    compacity_value = compacity(snake_list)\n",
    "    #accessible points \n",
    "    accessible_points_proportion = find_accessible_points(snake_list)-1\n",
    "    episode_length_penalty = -episode_length/(width*height//block_size**2)\n",
    "    penalties = np.array([accessible_points_proportion,penalty_distance,penalty_touch_self,penalty_distance*gass_reward,reward_eat,penalty_wall,penalty_danger,compacity_value,episode_length_penalty])\n",
    "  \n",
    "    penalty_names  = ['accessible_points_proportion','penalty_distance','penalty_touch_self','penalty_distance*gass_reward','reward_eat','penalty_wall','penalty_danger','compacity','episode_len_penalty']\n",
    "\n",
    "\n",
    "    total_reward = penalties@c/c.sum() \n",
    "\n",
    "    return total_reward\n",
    "\n",
    "def compacity(snake_list):\n",
    "    snake_list = np.array(snake_list)\n",
    "    min_x = snake_list[:,0].min()\n",
    "    min_y = snake_list[:,1].min()\n",
    "    max_x = snake_list[:,0].max()\n",
    "    max_y = snake_list[:,1].max()\n",
    "    return((max_y-min_y+block_size)*(max_x-min_x+block_size)/(len(snake_list)*block_size**2))   \n",
    "\n",
    "\n",
    "#if all cells are accessible \n",
    "def find_accessible_points(snake_list):\n",
    "    accessible_points= np.zeros((height//block_size,width//block_size))\n",
    "    head_position = snake_list[-1]\n",
    "    explore = [head_position]\n",
    "\n",
    "    while(len(explore)>0):\n",
    "        p = explore.pop()\n",
    "        accessible_points[p[1]//block_size,p[0]//block_size]=1\n",
    "        for m in actions.values():\n",
    "            (u,v)=(m[1]*block_size+p[0],m[0]*block_size+p[1])\n",
    "            if(inBounds(u,v)):\n",
    "                if(accessible_points[v//block_size,u//block_size]==0):\n",
    "                    if(not [u,v] in snake_list):\n",
    "                        explore.append((u,v))\n",
    "\n",
    "    return((np.sum(accessible_points)+len(snake_list)-1)*block_size**2/(height*width))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=802, out_features=4096, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (9): ReLU()\n",
      "  (10): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model, criterion, optimizer = initNNmodel()\n",
    "\n",
    "max_exploration_episodes = 150\n",
    "max_exploration_rate = 0.9\n",
    "min_exploration_rate = 0.05\n",
    "decay_rate = (min_exploration_rate/max_exploration_rate)**(1.0/max_exploration_episodes)\n",
    "#initialize deepQ\n",
    "dql = DQL(model, criterion, optimizer,actions.values(),decay_rate = decay_rate ,exploration_rate= max_exploration_rate ,device = device,epochs=10)\n",
    "# Initialize pygame\n",
    "\n",
    "food_x, food_y = generate_food([])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "\n",
    "def main(length,c,dql):\n",
    "    episode_reward = 0\n",
    "    # Initial snake position and food\n",
    "    snake_x = (width//block_size)//2     * block_size   \n",
    "    snake_y = (height//block_size)//2     * block_size   \n",
    "\n",
    "    snake_list = [[snake_x,snake_y]]\n",
    "    \n",
    "    global food_x,food_y\n",
    "    #food_x, food_y = generate_food([])   \n",
    "    episode_length =0 \n",
    "    # Initial snake direction and length\n",
    "\n",
    "    snake_length =length\n",
    "    #direction of the snake [0,1,2,3] each corresponding to one of up down left right\n",
    "    a=0\n",
    "    # = True if snake hits wall or itself\n",
    "    done = False\n",
    "    # number of collected fruit\n",
    "    score = 0\n",
    "    acts = list(actions.values())\n",
    "    # Game loop\n",
    "    check = 0 \n",
    "    while True:\n",
    "\n",
    "\n",
    "        St1 = state(snake_list,[food_x,food_y]) \n",
    "        a= dql.get_action(St1,a,snake_list,block_size,width,height)\n",
    "        r = reward(a,snake_list,episode_length-check,c)\n",
    "\n",
    "        #move snake\n",
    "        snake_x+=acts[a][1]*block_size\n",
    "        snake_y+=acts[a][0]*block_size\n",
    "        # Add new block of snake to the list\n",
    "        snake_list.append([snake_x, snake_y])\n",
    "        # Keep the length of snake same as snake_length\n",
    "        if len(snake_list) > snake_length:\n",
    "            del snake_list[0]\n",
    "\n",
    "        # Check if snake hits the boundaries\n",
    "        if snake_x >= width or snake_x < 0 or snake_y >= height or snake_y < 0:\n",
    "            done = True\n",
    "        # Check if snake hits itself\n",
    "        for block in snake_list[:-1]:\n",
    "            if block[0] == snake_x and block[1] == snake_y:\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "        St2 = state(snake_list,[food_x,food_y])\n",
    "        #add to Buffer\n",
    "        episode_reward= dql.add_memory(St1,a,r,St2,done,episode_reward)\n",
    "\n",
    "        # Check if snake hits the food\n",
    "        if snake_x == food_x and snake_y == food_y:\n",
    "            lens.append(episode_length-check)\n",
    "            food_x, food_y = generate_food(snake_list)\n",
    "            snake_length += 1\n",
    "            score+=1\n",
    "            check = episode_length\n",
    "        episode_length+=1\n",
    "        if((episode_length-check)%((width*height//block_size**2)) ==0 ):\n",
    "            dql.train(episode_length-check)\n",
    "        if((episode_length-check)%(3*(width*height//block_size**2)) ==0 ):\n",
    "            done = True\n",
    "        #if snake has hit something quit\n",
    "        if(done):\n",
    "            return [snake_length,episode_length,score,episode_reward]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4 0.5 0.3 0.1 0.6 0.3 0.2 0.1 0.4]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "max_m = 0\n",
    "max_avg =0 \n",
    "file_name = 'tuning_c_NOCNN_avg.txt'\n",
    "if(os.path.exists(file_name)):\n",
    "    with open(file_name, 'r') as f:\n",
    "\n",
    "        last_line = f.readlines()[-1]\n",
    "        m,max_avg_str,cs = last_line.split(\" \")\n",
    "        c=  np.array( [float(num) for num in cs.split(\",\")])\n",
    "        max_m = float(m)\n",
    "        max_avg = float(max_avg_str)\n",
    "best_c = deepcopy(c)\n",
    "print(best_c)\n",
    "print(max_avg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05555555555555556\n"
     ]
    }
   ],
   "source": [
    "#number of episodes\n",
    "num_exploration_episodes =1\n",
    "num_episodes = 100\n",
    "#maximum score reached\n",
    "m =0 \n",
    "#initial max length for the snake at birth\n",
    "max_length = 1\n",
    "#maximum allowed length for a snake \n",
    "max_max_length = width*height//block_size**2\n",
    "exploration_rate = 0.9\n",
    "min_exploration_rate = 0.05\n",
    "decay_rate = (min_exploration_rate/exploration_rate)**(1.0/num_exploration_episodes)\n",
    "print(decay_rate)\n",
    "def run_new_model(c,num_episodes,thread):\n",
    "    m=0\n",
    "    avg = 0 \n",
    "    global max_length,max_m,max_avg\n",
    "\n",
    "    model, criterion, optimizer = initNNmodel()\n",
    "    dql = DQL(model, criterion, optimizer,actions.values(),decay_rate = decay_rate,device = device)\n",
    "    for i in range(num_episodes):\n",
    "        print(thread,\" \",i,m,avg, max_m)\n",
    "        #do a generation and see the outcome\n",
    "        a= main(np.random.randint(1,max_length+1),c,dql)\n",
    "        #update maximum score \n",
    "        m = max(a[2],m)\n",
    "        avg = (1.0*i*avg+a[2])/(i+1)\n",
    "        #generate a new food position every 20 generations\n",
    "        if(i%2 ==0):\n",
    "            food_x, food_y = generate_food([])   \n",
    "\n",
    "        dql.train(a[1])\n",
    "\n",
    "    del model\n",
    "    del dql\n",
    "    if(avg>max_avg):\n",
    "\n",
    "        best_c = deepcopy(c)\n",
    "        with open(file_name, \"a\") as f:\n",
    "            f.write(\"\\n\")  # add a newline character before writing the list\n",
    "            f.write(str(m)+\" \")\n",
    "            f.write(str(avg)+ \" \")\n",
    "            f.write(\",\".join([str(k) for k in best_c])) \n",
    "        max_m = m\n",
    "        max_avg = avg\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "Sequential(\n",
      "  (0): Linear(in_features=802, out_features=4096, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (9): ReLU()\n",
      "  (10): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "0   0 0 0 0\n",
      "Sequential(\n",
      "  (0): Linear(in_features=802, out_features=4096, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (9): ReLU()\n",
      "  (10): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "3   0 0 0 0\n",
      "Sequential(\n",
      "  (0): Linear(in_features=802, out_features=4096, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (9): ReLU()\n",
      "  (10): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "2   0 0 0 0\n",
      "Sequential(\n",
      "  (0): Linear(in_features=802, out_features=4096, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (9): ReLU()\n",
      "  (10): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "1   0 0 0 0\n",
      "Sequential(\n",
      "  (0): Linear(in_features=802, out_features=4096, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (9): ReLU()\n",
      "  (10): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "4   0 0 0 0\n",
      "Sequential(\n",
      "  (0): Linear(in_features=802, out_features=4096, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (9): ReLU()\n",
      "  (10): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "5   0 0 0 0\n",
      "1   1 0 0.0 0\n",
      "3   1 4 4.0 0\n",
      "2   1 7 7.0 0\n",
      "4   1 1 1.0 0\n",
      "5   1 7 7.0 0\n",
      "1   2 1 0.5 0\n",
      "0   1 6 6.0 0\n",
      "3   2 4 2.0 0\n",
      "4   2 1 0.5 0\n",
      "2   2 7 3.5 0\n",
      "5   2 7 3.5 0\n",
      "1   3 1 0.3333333333333333 0\n",
      "0   2 6 3.0 0\n",
      "4   3 1 0.3333333333333333 0\n",
      "2   3 7 2.3333333333333335 0\n",
      "3   3 4 1.6666666666666667 0\n",
      "5   3 9 5.333333333333333 0\n",
      "0   3 6 2.3333333333333335 0\n",
      "4   4 7 2.0 0\n",
      "1   4 3 1.0 0\n",
      "4   5 7 2.0 0\n",
      "5   4 9 4.0 0\n",
      "0   4 6 2.0 0\n",
      "3   4 9 3.5 0\n",
      "1   5 4 1.6 0\n",
      "2   4 10 4.25 0\n",
      "0   5 6 2.4 0\n",
      "5   5 9 3.4 0\n",
      "4   6 7 2.5 0\n",
      "0   6 6 2.3333333333333335 0\n",
      "2   5 10 4.6 0\n",
      "2   6 10 4.166666666666667 0\n",
      "3   5 9 4.4 0\n",
      "1   6 4 1.5 0\n",
      "2   7 10 3.857142857142857 0\n",
      "5   6 9 3.8333333333333335 0\n",
      "4   7 7 2.5714285714285716 0\n",
      "0   7 6 2.142857142857143 0\n",
      "0   8 6 2.125 0\n",
      "1   7 4 1.2857142857142858 0\n",
      "5   7 9 3.4285714285714284 0\n",
      "3   6 9 5.0 0\n",
      "0   9 6 2.5555555555555554 0\n",
      "2   8 10 4.625 0\n",
      "1   8 4 1.25 0\n",
      "5   8 9 3.0 0\n",
      "2   9 10 5.222222222222222 0\n",
      "4   8 7 3.125 0\n",
      "3   7 9 4.571428571428571 0\n",
      "3   8 9 4.25 0\n",
      "0   10 6 2.3 0\n",
      "1   9 4 1.4444444444444444 0\n",
      "4   9 7 2.7777777777777777 0\n",
      "3   9 10 4.888888888888889 0\n",
      "0   11 6 2.1818181818181817 0\n",
      "5   9 9 3.3333333333333335 0\n",
      "1   10 4 1.4 0\n",
      "2   10 16 6.3 0\n",
      "0   12 6 2.0833333333333335 0\n",
      "2   11 16 6.090909090909091 0\n",
      "4   10 7 2.7 0\n",
      "3   10 10 5.2 0\n",
      "5   10 9 3.8 0\n",
      "1   11 4 1.3636363636363635 0\n",
      "4   11 7 3.0 0\n",
      "5   11 9 3.8181818181818183 0\n",
      "3   11 10 5.2727272727272725 0\n",
      "0   13 6 2.3076923076923075 0\n",
      "1   12 7 1.8333333333333333 0\n",
      "2   12 16 6.333333333333333 0\n",
      "3   12 10 5.5 0\n",
      "4   12 7 3.0833333333333335 0\n",
      "5   12 9 3.5 0\n",
      "4   13 7 3.0 0\n",
      "0   14 6 2.142857142857143 0\n",
      "1   13 7 1.6923076923076923 0\n",
      "2   13 16 5.846153846153846 0\n",
      "4   14 7 3.2857142857142856 0\n",
      "1   14 7 1.7142857142857142 0\n",
      "4   15 7 3.2 0\n",
      "5   13 9 3.3076923076923075 0\n",
      "3   13 10 5.6923076923076925 0\n",
      "2   14 16 6.071428571428571 0\n",
      "5   14 9 3.2142857142857144 0\n",
      "0   15 6 2.2666666666666666 0\n",
      "4   16 7 3.25 0\n",
      "2   15 16 6.133333333333334 0\n",
      "1   15 7 1.6666666666666667 0\n",
      "2   16 16 5.875 0\n",
      "3   14 10 5.357142857142857 0\n",
      "0   16 6 2.1875 0\n",
      "5   15 9 3.533333333333333 0\n",
      "4   17 7 3.4705882352941178 0\n",
      "3   15 10 5.0 0\n",
      "0   17 7 2.4705882352941178 0\n",
      "2   17 16 6.0588235294117645 0\n",
      "2   18 16 6.055555555555555 0\n",
      "5   16 9 3.3125 0\n",
      "3   16 10 5.1875 0\n",
      "0   18 7 2.4444444444444446 0\n",
      "1   16 7 1.9375 0\n",
      "4   18 7 3.5 0\n",
      "2   19 16 5.947368421052632 0\n",
      "5   17 9 3.176470588235294 0\n",
      "1   17 7 1.8235294117647058 0\n",
      "2   20 16 5.95 0\n",
      "2   21 16 5.761904761904762 0\n",
      "3   17 10 5.352941176470588 0\n",
      "0   19 7 2.6842105263157894 0\n",
      "2   22 16 5.590909090909091 0\n",
      "4   19 8 3.736842105263158 0\n",
      "5   18 9 3.388888888888889 0\n",
      "3   18 12 5.722222222222222 0\n",
      "2   23 16 5.608695652173913 0\n",
      "1   18 7 1.8888888888888888 0\n",
      "5   19 9 3.5789473684210527 0\n",
      "4   20 8 3.85 0\n",
      "2   24 16 5.666666666666667 0\n",
      "1   19 7 2.0526315789473686 0\n",
      "3   19 12 5.894736842105263 0\n",
      "5   20 9 3.75 0\n",
      "0   20 7 2.9 0\n",
      "0   21 7 2.857142857142857 0\n",
      "4   21 9 4.095238095238095 0\n",
      "1   20 7 2.05 0\n",
      "2   25 16 5.88 0\n",
      "5   21 9 3.619047619047619 0\n",
      "2   26 16 5.961538461538462 0\n",
      "4   22 9 4.2727272727272725 0\n",
      "3   20 12 5.85 0\n",
      "1   21 7 2.0476190476190474 0\n",
      "4   23 9 4.173913043478261 0\n",
      "2   27 16 6.111111111111111 0\n",
      "0   22 9 3.1363636363636362 0\n",
      "4   24 9 4.083333333333333 0\n",
      "2   28 16 5.964285714285714 0\n",
      "4   25 9 4.0 0\n",
      "2   29 16 5.896551724137931 0\n",
      "5   22 9 3.8636363636363638 0\n",
      "3   21 12 5.9523809523809526 0\n",
      "2   30 16 5.966666666666667 0\n",
      "4   26 9 3.8846153846153846 0\n",
      "3   22 12 5.954545454545454 0\n",
      "2   31 16 6.064516129032258 0\n",
      "0   23 9 3.3043478260869565 0\n",
      "1   22 7 2.227272727272727 0\n",
      "4   27 9 3.888888888888889 0\n",
      "2   32 16 6.125 0\n",
      "3   23 12 6.0 0\n",
      "5   23 9 3.9565217391304346 0\n",
      "3   24 12 5.833333333333333 0\n",
      "3   25 12 5.68 0\n",
      "3   26 12 5.538461538461538 0\n",
      "5   24 9 3.9583333333333335 0\n",
      "0   24 9 3.4583333333333335 0\n",
      "1   23 7 2.3913043478260865 0\n",
      "2   33 16 6.121212121212121 0\n",
      "5   25 9 3.88 0\n",
      "4   28 9 4.035714285714286 0\n",
      "2   34 16 6.147058823529412 0\n",
      "5   26 9 3.8076923076923075 0\n",
      "0   25 9 3.56 0\n",
      "2   35 16 6.2 0\n",
      "4   29 9 4.137931034482759 0\n",
      "0   26 9 3.6538461538461537 0\n",
      "1   24 7 2.4999999999999996 0\n",
      "3   27 12 5.592592592592593 0\n",
      "2   36 16 6.083333333333333 0\n",
      "3   28 12 5.75 0\n",
      "5   27 9 3.925925925925926 0\n",
      "1   25 7 2.3999999999999995 0\n",
      "2   37 16 6.216216216216216 0\n",
      "2   38 16 6.105263157894736 0\n",
      "4   30 9 4.3 0\n",
      "0   27 9 3.7777777777777777 0\n",
      "4   31 9 4.354838709677419 0\n",
      "0   28 9 3.7142857142857144 0\n",
      "5   28 9 3.7857142857142856 0\n",
      "2   39 16 6.179487179487179 0\n",
      "0   29 9 3.6551724137931036 0\n",
      "3   29 12 5.724137931034483 0\n",
      "0   30 9 3.6 0\n",
      "2   40 16 6.225 0\n",
      "1   26 7 2.576923076923076 0\n",
      "0   31 9 3.6129032258064515 0\n",
      "5   29 9 3.9655172413793105 0\n",
      "3   30 12 5.8 0\n",
      "2   41 16 6.365853658536586 0\n",
      "5   30 9 3.9 0\n",
      "4   32 9 4.375 0\n",
      "2   42 16 6.261904761904762 0\n",
      "4   33 9 4.303030303030303 0\n",
      "1   27 7 2.481481481481481 0\n",
      "3   31 12 5.870967741935484 0\n",
      "3   32 12 5.75 0\n",
      "2   43 16 6.348837209302325 0\n",
      "0   32 10 3.8125 0\n",
      "4   34 9 4.411764705882353 0\n",
      "3   33 12 5.636363636363637 0\n",
      "2   44 16 6.386363636363637 0\n",
      "5   31 9 3.967741935483871 0\n",
      "1   28 8 2.678571428571428 0\n",
      "4   35 9 4.457142857142857 0\n",
      "2   45 16 6.288888888888889 0\n",
      "4   36 9 4.388888888888889 0\n",
      "3   34 12 5.794117647058823 0\n",
      "2   46 16 6.195652173913044 0\n",
      "1   29 8 2.6551724137931028 0\n",
      "0   33 10 3.9393939393939394 0\n",
      "4   37 9 4.45945945945946 0\n",
      "3   35 12 5.857142857142857 0\n",
      "2   47 16 6.23404255319149 0\n",
      "2   48 16 6.145833333333333 0\n",
      "3   36 12 5.861111111111111 0\n",
      "0   34 10 3.8823529411764706 0\n",
      "3   37 12 5.756756756756757 0\n",
      "3   38 12 5.657894736842105 0\n",
      "4   38 9 4.526315789473684 0\n",
      "2   49 16 6.142857142857143 0\n",
      "0   35 10 3.8285714285714287 0\n",
      "0   36 10 3.7777777777777777 0\n",
      "5   32 11 4.1875 0\n",
      "4   39 9 4.461538461538462 0\n",
      "3   39 12 5.6923076923076925 0\n",
      "1   30 9 2.8666666666666663 0\n",
      "5   33 11 4.121212121212121 0\n",
      "3   40 12 5.675 0\n",
      "2   50 16 6.24 0\n",
      "4   40 9 4.475 0\n",
      "4   41 9 4.414634146341464 0\n",
      "1   31 9 2.9999999999999996 0\n",
      "3   41 12 5.682926829268292 0\n",
      "2   51 16 6.313725490196078 0\n",
      "5   34 11 4.0 0\n",
      "3   42 12 5.595238095238095 0\n",
      "0   37 10 3.7837837837837838 0\n",
      "3   43 12 5.511627906976744 0\n",
      "4   42 9 4.500000000000001 0\n",
      "2   52 16 6.3076923076923075 0\n",
      "0   38 10 3.8947368421052633 0\n",
      "5   35 11 3.9714285714285715 0\n",
      "1   32 9 2.9374999999999996 0\n",
      "0   39 10 3.8461538461538463 0\n",
      "2   53 16 6.339622641509434 0\n",
      "4   43 9 4.58139534883721 0\n",
      "3   44 12 5.590909090909091 0\n",
      "0   40 10 3.8 0\n",
      "5   36 11 4.027777777777778 0\n",
      "3   45 12 5.622222222222222 0\n",
      "0   41 10 3.8048780487804876 0\n",
      "3   46 12 5.5434782608695645 0\n",
      "1   33 9 2.8787878787878785 0\n",
      "3   47 12 5.468085106382978 0\n",
      "2   54 16 6.444444444444445 0\n",
      "0   42 10 3.761904761904762 0\n",
      "5   37 11 3.918918918918919 0\n",
      "4   44 10 4.704545454545456 0\n",
      "5   38 11 3.9210526315789473 0\n",
      "5   39 11 3.871794871794872 0\n",
      "5   40 11 3.825 0\n",
      "1   34 9 2.852941176470588 0\n",
      "3   48 12 5.520833333333333 0\n",
      "2   55 16 6.545454545454546 0\n",
      "5   41 11 3.8292682926829267 0\n",
      "4   45 10 4.7777777777777795 0\n",
      "0   43 10 3.813953488372093 0\n",
      "2   56 16 6.571428571428571 0\n",
      "2   57 16 6.491228070175438 0\n",
      "1   35 9 2.771428571428571 0\n",
      "4   46 10 4.847826086956523 0\n",
      "5   42 11 3.857142857142857 0\n",
      "2   58 16 6.5344827586206895 0\n",
      "3   49 12 5.530612244897959 0\n",
      "4   47 10 4.95744680851064 0\n",
      "2   59 16 6.576271186440678 0\n",
      "0   44 10 3.8863636363636362 0\n",
      "5   43 11 3.883720930232558 0\n",
      "5   44 11 3.840909090909091 0\n",
      "4   48 10 4.979166666666668 0\n",
      "1   36 9 2.8888888888888884 0\n",
      "2   60 16 6.6 0\n",
      "3   50 12 5.5 0\n",
      "3   51 12 5.431372549019608 0\n",
      "5   45 11 3.933333333333333 0\n",
      "0   45 10 3.933333333333333 0\n",
      "4   49 10 5.0612244897959195 0\n",
      "2   61 16 6.639344262295082 0\n",
      "4   50 10 5.120000000000001 0\n",
      "5   46 11 4.021739130434782 0\n",
      "3   52 12 5.403846153846154 0\n",
      "0   46 10 3.9782608695652173 0\n",
      "2   62 16 6.741935483870968 0\n",
      "2   63 16 6.666666666666667 0\n",
      "2   64 16 6.59375 0\n",
      "5   47 11 4.0212765957446805 0\n",
      "4   51 10 5.019607843137256 0\n",
      "2   65 16 6.523076923076923 0\n",
      "1   37 9 3.0270270270270268 0\n",
      "3   53 12 5.415094339622642 0\n",
      "5   48 11 4.020833333333333 0\n",
      "0   47 10 4.042553191489362 0\n",
      "2   66 16 6.606060606060606 0\n",
      "1   38 9 2.9736842105263155 0\n",
      "3   54 12 5.462962962962963 0\n",
      "5   49 11 3.938775510204082 0\n",
      "4   52 11 5.134615384615386 0\n",
      "2   67 16 6.626865671641791 0\n",
      "0   48 10 4.0625 0\n",
      "2   68 16 6.647058823529412 0\n",
      "3   55 12 5.527272727272727 0\n",
      "3   56 12 5.464285714285714 0\n",
      "4   53 11 5.245283018867926 0\n",
      "1   39 9 3.0769230769230766 0\n",
      "4   54 11 5.277777777777779 0\n",
      "4   55 11 5.21818181818182 0\n",
      "5   50 11 4.02 0\n",
      "2   69 16 6.72463768115942 0\n",
      "2   70 16 6.685714285714286 0\n",
      "3   57 12 5.508771929824562 0\n",
      "0   49 10 4.061224489795919 0\n",
      "2   71 16 6.676056338028169 0\n",
      "0   50 10 4.02 0\n",
      "2   72 16 6.611111111111111 0\n",
      "1   40 9 2.9999999999999996 0\n",
      "5   51 11 4.019607843137254 0\n",
      "2   73 16 6.575342465753424 0\n",
      "2   74 16 6.513513513513513 0\n",
      "3   58 12 5.568965517241379 0\n",
      "0   51 10 4.078431372549019 0\n",
      "2   75 16 6.453333333333333 0\n",
      "5   52 11 4.076923076923076 0\n",
      "4   56 11 5.30357142857143 0\n",
      "0   52 10 4.115384615384615 0\n",
      "4   57 11 5.24561403508772 0\n",
      "1   41 9 2.9512195121951215 0\n",
      "3   59 12 5.627118644067797 0\n",
      "5   53 11 4.037735849056602 0\n",
      "2   76 16 6.473684210526316 0\n",
      "5   54 11 4.092592592592592 0\n",
      "2   77 16 6.5064935064935066 0\n",
      "1   42 9 2.8809523809523805 0\n",
      "4   58 11 5.293103448275863 0\n",
      "2   78 16 6.576923076923077 0\n",
      "0   53 10 4.169811320754716 0\n",
      "2   79 16 6.518987341772152 0\n",
      "2   80 16 6.4625 0\n",
      "3   60 12 5.683333333333334 0\n",
      "5   55 11 4.036363636363635 0\n",
      "4   59 11 5.271186440677967 0\n",
      "1   43 9 2.9534883720930227 0\n",
      "4   60 11 5.216666666666668 0\n",
      "3   61 12 5.737704918032787 0\n",
      "2   81 16 6.518518518518518 0\n",
      "5   56 11 4.07142857142857 0\n",
      "3   62 12 5.774193548387097 0\n",
      "0   54 10 4.222222222222221 0\n",
      "0   55 10 4.181818181818181 0\n",
      "1   44 9 3.0681818181818175 0\n",
      "3   63 12 5.746031746031746 0\n",
      "3   64 12 5.6875 0\n",
      "1   45 9 3.044444444444444 0\n",
      "0   56 10 4.1428571428571415 0\n",
      "4   61 11 5.278688524590165 0\n",
      "5   57 11 4.140350877192981 0\n",
      "2   82 16 6.548780487804878 0\n",
      "4   62 11 5.225806451612904 0\n",
      "5   58 11 4.172413793103447 0\n",
      "5   59 11 4.135593220338982 0\n",
      "1   46 9 2.978260869565217 0\n",
      "3   65 12 5.753846153846154 0\n",
      "2   83 16 6.518072289156627 0\n",
      "3   66 12 5.696969696969697 0\n",
      "3   67 12 5.641791044776119 0\n",
      "5   60 11 4.166666666666665 0\n",
      "0   57 10 4.175438596491227 0\n",
      "2   84 16 6.559523809523809 0\n",
      "3   68 12 5.676470588235294 0\n",
      "4   63 11 5.301587301587302 0\n",
      "3   69 12 5.6231884057971016 0\n",
      "3   70 12 5.571428571428571 0\n",
      "2   85 16 6.588235294117647 0\n",
      "0   58 10 4.2413793103448265 0\n",
      "3   71 12 5.52112676056338 0\n",
      "1   47 9 3.0851063829787226 0\n",
      "3   72 12 5.5 0\n",
      "4   64 11 5.343750000000001 0\n",
      "5   61 11 4.2295081967213095 0\n",
      "5   62 11 4.241935483870966 0\n",
      "4   65 11 5.292307692307693 0\n",
      "3   73 12 5.575342465753424 0\n",
      "4   66 11 5.242424242424243 0\n",
      "2   86 16 6.604651162790698 0\n",
      "5   63 11 4.3015873015873 0\n",
      "1   48 9 3.041666666666666 0\n",
      "3   74 12 5.608108108108108 0\n",
      "3   75 12 5.56 0\n",
      "2   87 16 6.620689655172414 0\n",
      "0   59 10 4.338983050847457 0\n",
      "3   76 12 5.578947368421052 0\n",
      "4   67 11 5.268656716417912 0\n",
      "2   88 16 6.6477272727272725 0\n",
      "3   77 12 5.5974025974025965 0\n",
      "5   64 11 4.390624999999998 0\n",
      "1   49 9 3.0816326530612237 0\n",
      "2   89 16 6.662921348314606 0\n",
      "4   68 11 5.294117647058824 0\n",
      "0   60 10 4.366666666666665 0\n",
      "2   90 16 6.677777777777778 0\n",
      "3   78 12 5.628205128205128 0\n",
      "5   65 11 4.46153846153846 0\n",
      "5   66 11 4.424242424242422 0\n",
      "2   91 16 6.7032967032967035 0\n",
      "4   69 11 5.376811594202899 0\n",
      "5   67 11 4.388059701492535 0\n",
      "2   92 16 6.760869565217392 0\n",
      "4   70 11 5.42857142857143 0\n",
      "1   50 9 3.1599999999999993 0\n",
      "0   61 10 4.442622950819671 0\n",
      "2   93 16 6.763440860215054 0\n",
      "2   94 16 6.712765957446808 0\n",
      "3   79 12 5.683544303797468 0\n",
      "4   71 11 5.380281690140846 0\n",
      "5   68 11 4.470588235294116 0\n",
      "3   80 12 5.637499999999999 0\n",
      "4   72 11 5.333333333333334 0\n",
      "4   73 11 5.287671232876713 0\n",
      "2   95 16 6.6947368421052635 0\n",
      "5   69 11 4.507246376811593 0\n",
      "0   62 10 4.46774193548387 0\n",
      "1   51 9 3.1764705882352935 0\n",
      "4   74 11 5.297297297297298 0\n",
      "3   81 12 5.679012345679012 0\n",
      "5   70 11 4.599999999999999 0\n",
      "1   52 9 3.2307692307692304 0\n",
      "2   96 16 6.71875 0\n",
      "5   71 11 4.619718309859153 0\n",
      "4   75 11 5.3066666666666675 0\n",
      "5   72 11 4.583333333333332 0\n",
      "2   97 16 6.752577319587629 0\n",
      "2   98 16 6.704081632653061 0\n",
      "0   63 10 4.507936507936507 0\n",
      "2   99 16 6.6767676767676765 0\n",
      "3   82 12 5.7073170731707314 0\n",
      "5   73 11 4.602739726027396 16\n",
      "5   74 11 4.567567567567567 16\n",
      "4   76 11 5.250000000000001 16\n",
      "3   83 12 5.734939759036145 16\n",
      "4   77 11 5.233766233766234 16\n",
      "0   64 10 4.531249999999999 16\n",
      "1   53 9 3.3207547169811313 16\n",
      "4   78 11 5.294871794871795 16\n",
      "3   84 12 5.785714285714286 16\n",
      "5   75 11 4.6 16\n",
      "1   54 9 3.296296296296296 16\n",
      "0   65 10 4.569230769230768 16\n",
      "0   66 10 4.530303030303029 16\n",
      "3   85 12 5.788235294117647 16\n",
      "5   76 11 4.657894736842105 16\n",
      "4   79 11 5.30379746835443 16\n",
      "3   86 12 5.813953488372093 16\n",
      "1   55 9 3.254545454545454 16\n",
      "0   67 10 4.53731343283582 16\n",
      "4   80 11 5.3625 16\n",
      "5   77 11 4.701298701298701 16\n",
      "3   87 12 5.839080459770115 16\n",
      "5   78 11 4.666666666666667 16\n",
      "4   81 11 5.320987654320987 16\n",
      "0   68 10 4.558823529411764 16\n",
      "5   79 11 4.658227848101266 16\n",
      "1   56 9 3.2678571428571423 16\n",
      "3   88 12 5.875 16\n",
      "0   69 10 4.623188405797101 16\n",
      "4   82 11 5.390243902439025 16\n",
      "5   80 11 4.7 16\n",
      "3   89 12 5.921348314606742 16\n",
      "3   90 12 5.877777777777778 16\n",
      "3   91 12 5.835164835164835 16\n",
      "1   57 9 3.228070175438596 16\n",
      "3   92 12 5.793478260869565 16\n",
      "4   83 11 5.421686746987952 16\n",
      "0   70 10 4.671428571428571 16\n",
      "5   81 11 4.753086419753086 16\n",
      "4   84 11 5.416666666666667 16\n",
      "3   93 12 5.806451612903226 16\n",
      "0   71 10 4.676056338028168 16\n",
      "1   58 9 3.2931034482758617 16\n",
      "0   72 10 4.6805555555555545 16\n",
      "3   94 12 5.851063829787234 16\n",
      "5   82 11 4.7682926829268295 16\n",
      "4   85 11 5.447058823529412 16\n",
      "3   95 12 5.863157894736842 16\n",
      "5   83 11 4.783132530120482 16\n",
      "4   86 11 5.465116279069767 16\n",
      "0   73 10 4.6986301369863 16\n",
      "4   87 11 5.448275862068965 16\n",
      "3   96 12 5.916666666666667 16\n",
      "1   59 9 3.35593220338983 16\n",
      "5   84 11 4.857142857142857 16\n",
      "3   97 12 5.948453608247423 16\n",
      "4   88 11 5.5 16\n",
      "0   74 10 4.716216216216215 16\n",
      "5   85 11 4.823529411764706 16\n",
      "3   98 12 5.938775510204081 16\n",
      "4   89 11 5.52808988764045 16\n",
      "1   60 9 3.4333333333333327 16\n",
      "4   90 11 5.488888888888889 16\n",
      "0   75 10 4.773333333333332 16\n",
      "5   86 11 4.825581395348837 16\n",
      "1   61 9 3.40983606557377 16\n",
      "3   99 12 5.959595959595959 16\n",
      "4   91 11 5.472527472527473 16\n",
      "0   76 10 4.789473684210525 16\n",
      "5   87 11 4.873563218390805 16\n",
      "0   77 10 4.83116883116883 16\n",
      "5   88 11 4.840909090909091 16\n",
      "4   92 11 5.5 16\n",
      "1   62 9 3.4354838709677415 16\n",
      "0   78 10 4.858974358974358 16\n",
      "0   79 10 4.8227848101265804 16\n",
      "4   93 11 5.505376344086022 16\n",
      "5   89 11 4.898876404494382 16\n",
      "1   63 9 3.46031746031746 16\n",
      "4   94 11 5.51063829787234 16\n",
      "0   80 11 4.899999999999998 16\n",
      "5   90 11 4.966666666666668 16\n",
      "1   64 9 3.4374999999999996 16\n",
      "0   81 11 4.864197530864195 16\n",
      "5   91 11 5.010989010989012 16\n",
      "4   95 11 5.5473684210526315 16\n",
      "4   96 11 5.510416666666667 16\n",
      "1   65 9 3.384615384615384 16\n",
      "4   97 11 5.474226804123711 16\n",
      "5   92 11 5.000000000000001 16\n",
      "4   98 11 5.479591836734694 16\n",
      "1   66 9 3.333333333333333 16\n",
      "4   99 11 5.444444444444445 16\n",
      "0   82 11 4.902439024390242 16\n",
      "5   93 11 5.053763440860216 16\n",
      "1   67 9 3.2985074626865667 16\n",
      "0   83 11 4.939759036144577 16\n",
      "5   94 11 5.095744680851064 16\n",
      "5   95 11 5.063157894736842 16\n",
      "5   96 11 5.09375 16\n",
      "0   84 11 4.952380952380951 16\n",
      "1   68 9 3.3235294117647056 16\n",
      "5   97 11 5.134020618556701 16\n",
      "0   85 11 4.952941176470587 16\n",
      "1   69 9 3.3913043478260865 16\n",
      "0   86 11 4.988372093023255 16\n",
      "1   70 9 3.4142857142857137 16\n",
      "5   98 11 5.173469387755103 16\n",
      "0   87 11 5.011494252873563 16\n",
      "0   88 11 5.0227272727272725 16\n",
      "1   71 9 3.4647887323943656 16\n",
      "5   99 11 5.202020202020202 16\n",
      "0   89 11 5.056179775280899 16\n",
      "0   90 11 5.122222222222223 16\n",
      "1   72 9 3.4583333333333326 16\n",
      "0   91 11 5.120879120879121 16\n"
     ]
    }
   ],
   "source": [
    "number_of_threads = 6\n",
    "\n",
    "from threading import Thread \n",
    "run = 0 \n",
    "borne = 0.5\n",
    "\n",
    "while(borne>0.05):\n",
    "    borne = 0.5*np.exp(-run/6)\n",
    "    print(borne)\n",
    "    threads = []\n",
    "    for i in range(number_of_threads):\n",
    "            # Create a thread for this combination of parameter values and start it\n",
    "        c= np.array([np.random.uniform(np.clip(x-borne,0,1),np.clip(x+borne,0,1)) for x in best_c])\n",
    "        t = Thread(target=run_new_model, args=(c, num_episodes,i))\n",
    "        t.start()\n",
    "        \n",
    "        # Add the thread to the list\n",
    "        threads.append(t)\n",
    "    # Wait for all the threads to finish\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    run+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.system('shutdown /s /t 0')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
