{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.2.0 (SDL 2.0.22, Python 3.8.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alhus\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pygame \n",
    "import random\n",
    "from copy import deepcopy\n",
    "import numpy as np \n",
    "import os \n",
    "import sys \n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Move the model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQL:\n",
    "    def __init__(self, model, criterion, optimizer, actions, discount_factor=0.6, exploration_rate=0.9, memory_size=100000, batch_size=500,base_decay_rate = 0.995, decay_rate=0.98565207, base_exploration_rate = 0.1,validation_batch_size = 100 ,epochs = 1,device = 'cpu'):\n",
    "        #NN\n",
    "        self.model = model\n",
    "        self.actions = actions\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        #gamma\n",
    "        self.discount_factor = discount_factor\n",
    "        #for epsilon-greedy\n",
    "        self.exploration_rate = exploration_rate\n",
    "        #buffer\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.evalmemory  = deque(maxlen = memory_size)\n",
    "        self.batch_size = batch_size\n",
    "        #diminish exploration \n",
    "        self.base_decay_rate = base_decay_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.validation_batch_size = validation_batch_size\n",
    "        self.base_exploration_rate = base_exploration_rate\n",
    "\n",
    "    def get_action(self, state, direction, snake_list, block_size, width, height):\n",
    "        action = direction\n",
    "        possible_moves = list(range(0, len(self.actions)))\n",
    "\n",
    "        # eliminate all impossible moves\n",
    "        acts = list(self.actions)\n",
    "        poss_copy = possible_moves.copy()\n",
    "        for p in poss_copy:\n",
    "            (u, v) = (snake_list[-1][0] + acts[p][1] * block_size, snake_list[-1][1] + acts[p][0] * block_size)\n",
    "            if (u < 0 or u >= width or v < 0 or v >= height or [u, v] in snake_list[1:]):\n",
    "                possible_moves.remove(p)\n",
    "\n",
    "        if len(possible_moves) > 0:\n",
    "            if np.random.rand() < self.base_exploration_rate + self.exploration_rate:\n",
    "                # Choose a random action\n",
    "                action = possible_moves[np.random.randint(len(possible_moves))]\n",
    "            else:\n",
    "                # Choose the best action according to the model\n",
    "                with torch.no_grad():\n",
    "                    q_values = self.model(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device))\n",
    "                    q_values = q_values.squeeze(0)\n",
    "\n",
    "                sorted = q_values.argsort(descending=True)\n",
    "                for s in sorted:\n",
    "                    if s in possible_moves:\n",
    "                        action = s\n",
    "                        break\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def add_memory(self, state, action, reward, next_state, done,episode_reward):\n",
    "        x = np.random.rand()\n",
    "\n",
    "        if(x<0.9):\n",
    "            self.memory.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.evalmemory.append((state, action, reward, next_state, done))\n",
    "        episode_reward=episode_reward*self.discount_factor+reward\n",
    "        return episode_reward\n",
    "    \n",
    "    def train(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            # Not enough memories to train the model\n",
    "            return\n",
    "\n",
    "        # Randomly sample memories from the replay buffer\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "\n",
    "        states = torch.tensor(states,dtype = torch.float32).to(self.device)\n",
    "\n",
    "\n",
    "        next_states = torch.tensor(next_states,dtype = torch.float32).to(self.device)\n",
    "\n",
    "\n",
    "        # Decrease the exploration rate\n",
    "        self.exploration_rate *= self.decay_rate\n",
    "        self.base_exploration_rate *= self.base_decay_rate\n",
    "\n",
    "        target_q_values = np.zeros((batch_size,len(self.actions)))\n",
    "\n",
    "        # Calculate the target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.model(next_states).to(self.device)\n",
    "            for i in range(batch_size):\n",
    "                if dones[i]:\n",
    "                    target_q_values[i][actions[i]] = rewards[i]\n",
    "\n",
    "                else:\n",
    "                    target_q_values[i][actions[i]] = rewards[i] + self.discount_factor * max(next_q_values[i])\n",
    "        target_q_values = torch.tensor(target_q_values,dtype = torch.float32).to(self.device)\n",
    "\n",
    "        # Train the model for a specified number of epochs\n",
    "        for _ in range(self.epochs):\n",
    "            predicted_q_values = self.model(states).float().to(self.device)\n",
    "            loss = nn.functional.mse_loss(predicted_q_values, target_q_values)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Remove variables from memory\n",
    "        del states, actions, rewards, next_states, dones, target_q_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Snake block size\n",
    "block_size = 25\n",
    "\n",
    "\n",
    "# Set display width and height\n",
    "width = 500 \n",
    "height = 500\n",
    "penalty_names  = ['accessible_points_proportion','penalty_distance','penalty_touch_self','penalty_distance*gass_reward','reward_eat','penalty_wall','penalty_danger','compacity','episode_len_penalty']\n",
    "c = np.array([0.4,0.5,0.3,0.1,0.6,0.3,0.2,0.1,0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_food(snake_list):\n",
    "    # Generate food for the snake where there is no snake\n",
    "    food_x, food_y = None, None\n",
    "    \n",
    "    while food_x is None or food_y is None or [food_x, food_y] in snake_list:\n",
    "        food_x = round(random.randrange(0, width - block_size) / block_size) * block_size\n",
    "        food_y = round(random.randrange(0, height - block_size) / block_size) * block_size\n",
    "    return food_x, food_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary of possible actions \n",
    "actions = {\"up\":(-1,0),\"down\":(1,0),\"left\":(0,-1),\"right\":(0,1)}\n",
    "\n",
    "#(x,y) apple + snake body (which has at most width*height parts) \n",
    "input_size = 2*width*height//block_size**2+2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initNNmodel():\n",
    "    # Define the input shape\n",
    "    input_shape = (3, height//block_size, width//block_size)\n",
    "\n",
    "    # Create a Sequential model\n",
    "    model = nn.Sequential(\n",
    "        # Add a 2D convolutional layer with 80 filters, a kernel size of 3x3, and ReLU activation\n",
    "        nn.Conv2d(3, 80, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        # Add a flatten layer to convert the 2D output to a 1D vector\n",
    "        nn.Flatten(),\n",
    "        \n",
    "        # Add 3 fully connected layers with ReLU activation\n",
    "        nn.Linear(80 * (height // block_size) * (width // block_size), 2048),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2048, 1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(1024, 512),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        # Add the output layer with len(actions) units\n",
    "        nn.Linear(512, len(actions))\n",
    "    )\n",
    "\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state(snake_list,apple):\n",
    "    layer_head = np.zeros((height//block_size,width//block_size))\n",
    "    layer_tail = np.zeros((height//block_size,width//block_size))\n",
    "    layer_apple = np.zeros((height//block_size,width//block_size))\n",
    "    layer_head[snake_list[-1][0]//block_size-1, snake_list[-1][1]//block_size-1] =1 \n",
    "    layer_apple[apple[0]//block_size-1, apple[1]//block_size-1] =1 \n",
    "    for s in snake_list[:-1]:\n",
    "        layer_tail[s[0]//block_size-1, s[1]//block_size-1] =1 \n",
    "    input = np.zeros( (3,height//block_size, width//block_size))\n",
    "    input[0,:,:] = layer_head\n",
    "    input[1,:,:] = layer_tail\n",
    "    input[2,:,:] = layer_apple\n",
    "\n",
    "    return(input)\n",
    "\n",
    "def normalized_distance(u,v,food_x,food_y):\n",
    "    return np.sqrt((((u-food_x)/width)**2+((v-food_y)/height)**2)/2)\n",
    "\n",
    "def inBounds(u,v):\n",
    "    if(u>=0 and v>=0):\n",
    "        if(u<width and v<height):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def gaussian_aroundone(x,alpha):\n",
    "    return(np.exp(-alpha*(x-1)**2))\n",
    "def danger_distance(direction, snake_list):\n",
    "    dis =0  \n",
    "    acts = list(actions.values())\n",
    "    (u,v) = (snake_list[-1][0],snake_list[-1][1])\n",
    "    while (inBounds(u,v)):\n",
    "        u +=block_size*acts[direction][1]\n",
    "        v+=block_size*acts[direction][0]\n",
    "        if([u,v] in snake_list[1:]):\n",
    "            return (-1+1.0*dis*block_size/max(width,height))\n",
    "        dis+=1\n",
    "    return(0)\n",
    "\n",
    "#reward function for each state and action\n",
    "def reward(action, snake_list,episode_length,c):\n",
    "    copy = deepcopy(snake_list)\n",
    "    p = copy[-1]\n",
    "    a = list(actions.values())\n",
    "    (u,v)=(a[action][1]*block_size+p[0],a[action][0]*block_size+p[1])\n",
    "    penalty_touch_self = 0 \n",
    "    if [u,v] in snake_list:\n",
    "        penalty_touch_self=-1 # return a negative reward if the snake collides with itself\n",
    "    copy.append([u,v])\n",
    "    del copy[0]\n",
    "    \n",
    "    global food_x, food_y\n",
    "    # reward the agent for getting closer to the food\n",
    "    reward_distance = 1-normalized_distance(u,v,food_x,food_y)\n",
    "    #if too far then the reward is very close to 0 \n",
    "    gass_reward =gaussian_aroundone(reward_distance,20)\n",
    "    # reward the agent for eating the food\n",
    "    reward_eat = 1 if u == food_x and v == food_y else 0\n",
    "    # penalize the agent for moving away from the food\n",
    "    penalty_distance = -1 if normalized_distance(u,v,food_x,food_y) > normalized_distance(p[0], p[1], food_x, food_y) else 0.5\n",
    "    # penalize he agent for hitting a wall\n",
    "    penalty_wall = -1 if not (inBounds(u,v)) else 0\n",
    "    #penalize the agent for getting closer to danger\n",
    "    penalty_danger = danger_distance(action,snake_list)\n",
    "    #print(penalty_danger)\n",
    "    compacity_value = 1/compacity(snake_list)\n",
    "    #accessible points \n",
    "    accessible_points_proportion = find_accessible_points(snake_list)-1\n",
    "    episode_length_penalty = -episode_length/(width*height//block_size**2)\n",
    "    penalties = np.array([accessible_points_proportion,penalty_distance,penalty_touch_self,penalty_distance*gass_reward,reward_eat,penalty_wall,penalty_danger,compacity_value,episode_length_penalty])\n",
    "  \n",
    "    penalty_names  = ['accessible_points_proportion','penalty_distance','penalty_touch_self','penalty_distance*gass_reward','reward_eat','penalty_wall','penalty_danger','compacity','episode_len_penalty']\n",
    "\n",
    "\n",
    "    total_reward = penalties@c/c.sum() \n",
    "\n",
    "    return total_reward\n",
    "\n",
    "def compacity(snake_list):\n",
    "    snake_list = np.array(snake_list)\n",
    "    min_x = snake_list[:,0].min()\n",
    "    min_y = snake_list[:,1].min()\n",
    "    max_x = snake_list[:,0].max()\n",
    "    max_y = snake_list[:,1].max()\n",
    "    return((max_y-min_y+block_size)*(max_x-min_x+block_size)/(len(snake_list)*block_size**2))   \n",
    "\n",
    "\n",
    "#if all cells are accessible \n",
    "def find_accessible_points(snake_list):\n",
    "    accessible_points= np.zeros((height//block_size,width//block_size))\n",
    "    head_position = snake_list[-1]\n",
    "    explore = [head_position]\n",
    "\n",
    "    while(len(explore)>0):\n",
    "        p = explore.pop()\n",
    "        accessible_points[p[1]//block_size,p[0]//block_size]=1\n",
    "        for m in actions.values():\n",
    "            (u,v)=(m[1]*block_size+p[0],m[0]*block_size+p[1])\n",
    "            if(inBounds(u,v)):\n",
    "                if(accessible_points[v//block_size,u//block_size]==0):\n",
    "                    if(not [u,v] in snake_list):\n",
    "                        explore.append((u,v))\n",
    "\n",
    "    return((np.sum(accessible_points)+len(snake_list)-1)*block_size**2/(height*width))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, criterion, optimizer = initNNmodel()\n",
    "\n",
    "max_exploration_episodes = 150\n",
    "max_exploration_rate = 0.9\n",
    "min_exploration_rate = 0.05\n",
    "decay_rate = (min_exploration_rate/max_exploration_rate)**(1.0/max_exploration_episodes)\n",
    "#initialize deepQ\n",
    "dql = DQL(model, criterion, optimizer,actions.values(),decay_rate = decay_rate ,exploration_rate= max_exploration_rate ,device = device,epochs=10)\n",
    "# Initialize pygame\n",
    "\n",
    "food_x, food_y = generate_food([])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "\n",
    "def main(length,c,dql):\n",
    "    episode_reward = 0\n",
    "    # Initial snake position and food\n",
    "    snake_x = (width//block_size)//2     * block_size   \n",
    "    snake_y = (height//block_size)//2     * block_size   \n",
    "\n",
    "    snake_list = [[snake_x,snake_y]]\n",
    "    \n",
    "    global food_x,food_y\n",
    "    #food_x, food_y = generate_food([])   \n",
    "    episode_length =0 \n",
    "    # Initial snake direction and length\n",
    "\n",
    "    snake_length =length\n",
    "    #direction of the snake [0,1,2,3] each corresponding to one of up down left right\n",
    "    a=0\n",
    "    # = True if snake hits wall or itself\n",
    "    done = False\n",
    "    # number of collected fruit\n",
    "    score = 0\n",
    "    acts = list(actions.values())\n",
    "    # Game loop\n",
    "    check = 0 \n",
    "    while True:\n",
    "\n",
    "\n",
    "        St1 = state(snake_list,[food_x,food_y]) \n",
    "        a= dql.get_action(St1,a,snake_list,block_size,width,height)\n",
    "        r = reward(a,snake_list,episode_length-check,c)\n",
    "\n",
    "        #move snake\n",
    "        snake_x+=acts[a][1]*block_size\n",
    "        snake_y+=acts[a][0]*block_size\n",
    "        # Add new block of snake to the list\n",
    "        snake_list.append([snake_x, snake_y])\n",
    "        # Keep the length of snake same as snake_length\n",
    "        if len(snake_list) > snake_length:\n",
    "            del snake_list[0]\n",
    "\n",
    "        # Check if snake hits the boundaries\n",
    "        if snake_x >= width or snake_x < 0 or snake_y >= height or snake_y < 0:\n",
    "            done = True\n",
    "        # Check if snake hits itself\n",
    "        for block in snake_list[:-1]:\n",
    "            if block[0] == snake_x and block[1] == snake_y:\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "        St2 = state(snake_list,[food_x,food_y])\n",
    "        #add to Buffer\n",
    "        episode_reward= dql.add_memory(St1,a,r,St2,done,episode_reward)\n",
    "\n",
    "        # Check if snake hits the food\n",
    "        if snake_x == food_x and snake_y == food_y:\n",
    "            lens.append(episode_length-check)\n",
    "            food_x, food_y = generate_food(snake_list)\n",
    "            snake_length += 1\n",
    "            score+=1\n",
    "            check = episode_length\n",
    "        episode_length+=1\n",
    "#\n",
    "        if((episode_length-check)%((width*height//block_size**2)) ==0 ):\n",
    "            done = True\n",
    "        #if snake has hit something quit\n",
    "        if(done):\n",
    "            return [snake_length,episode_length,score,episode_reward]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38697159 0.52548699 0.22038228 0.40087986 0.33995491 0.29913072\n",
      " 0.58751657 0.34641235 0.48963719]\n",
      "2.44\n"
     ]
    }
   ],
   "source": [
    "max_m = 0\n",
    "max_avg =0 \n",
    "if(os.path.exists('tuning_c_avg.txt')):\n",
    "    with open('tuning_c_avg.txt', 'r') as f:\n",
    "\n",
    "        last_line = f.readlines()[-1]\n",
    "        m,max_avg_str,cs = last_line.split(\" \")\n",
    "        c=  np.array( [float(num) for num in cs.split(\",\")])\n",
    "        max_m = float(m)\n",
    "        max_avg = float(max_avg_str)\n",
    "best_c = deepcopy(c)\n",
    "print(best_c)\n",
    "print(max_avg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.971509999298448\n"
     ]
    }
   ],
   "source": [
    "#number of episodes\n",
    "num_episodes =100\n",
    "#maximum score reached\n",
    "m =0 \n",
    "#initial max length for the snake at birth\n",
    "max_length = 1\n",
    "#maximum allowed length for a snake \n",
    "max_max_length = width*height//block_size**2\n",
    "exploration_rate = 0.9\n",
    "min_exploration_rate = 0.05\n",
    "decay_rate = (min_exploration_rate/exploration_rate)**(1.0/num_episodes)\n",
    "print(decay_rate)\n",
    "def run_new_model(c,num_episodes,thread):\n",
    "    m=0\n",
    "    avg = 0 \n",
    "    global max_length,max_m,max_avg\n",
    "\n",
    "    model, criterion, optimizer = initNNmodel()\n",
    "    dql = DQL(model, criterion, optimizer,actions.values(),decay_rate = decay_rate,device = device)\n",
    "    for i in range(num_episodes):\n",
    "        print(thread,\" \",i,m,avg, max_m)\n",
    "        #do a generation and see the outcome\n",
    "        a= main(np.random.randint(1,max_length+1),c,dql)\n",
    "        #update maximum score \n",
    "        m = max(a[2],m)\n",
    "        avg = (1.0*i*avg+a[2])/(i+1)\n",
    "        #generate a new food position every 20 generations\n",
    "        if(i%2 ==0):\n",
    "            food_x, food_y = generate_food([])   \n",
    "\n",
    "        dql.train(a[1])\n",
    "\n",
    "    del model\n",
    "    del dql\n",
    "    if(m>max_m or (m==max_m and avg>max_avg)):\n",
    "\n",
    "        best_c = deepcopy(c)\n",
    "        with open(\"tuning_c_avg.txt\", \"a\") as f:\n",
    "            f.write(\"\\n\")  # add a newline character before writing the list\n",
    "            f.write(str(m)+\" \")\n",
    "            f.write(str(avg)+ \" \")\n",
    "            f.write(\",\".join([str(k) for k in best_c])) \n",
    "        max_m = m\n",
    "        max_avg = avg\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "50   0 0 0 8.0\n",
      "   0 0 0 8.0\n",
      "4   0 0 0 8.0\n",
      "2   0 0 0 8.0\n",
      "1   0 0 0 8.0\n",
      "3   0 0 0 8.0\n",
      "1   1 0 0.0 8.0\n",
      "3   1 0 0.0 8.0\n",
      "0   1 0 0.0 8.0\n",
      "2   1 0 0.0 8.0\n",
      "5   1 0 0.0 8.0\n",
      "4   1 0 0.0 8.0\n",
      "0   2 0 0.0 8.0\n",
      "3   2 0 0.0 8.0\n",
      "5   2 0 0.0 8.0\n",
      "1   2 0 0.0 8.0\n",
      "4   2 1 0.5 8.0\n",
      "2   2 1 0.5 8.0\n",
      "1   3 0 0.0 8.0\n",
      "5   3 0 0.0 8.0\n",
      "3   3 1 0.3333333333333333 8.0\n",
      "0   3 2 0.6666666666666666 8.0\n",
      "4   3 1 0.3333333333333333 8.0\n",
      "2   3 1 0.3333333333333333 8.0\n",
      "1   4 0 0.0 8.0\n",
      "5   4 0 0.0 8.0\n",
      "0   4 2 0.5 8.0\n",
      "3   4 1 0.25 8.0\n",
      "4   4 1 0.25 8.0\n",
      "2   4 1 0.25 8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Alhus\\anaconda3\\envs\\pytorch\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\Alhus\\anaconda3\\envs\\pytorch\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Alhus\\AppData\\Local\\Temp\\ipykernel_6408\\3210863791.py\", line 31, in run_new_model\n",
      "  File \"C:\\Users\\Alhus\\AppData\\Local\\Temp\\ipykernel_6408\\2193369100.py\", line 109, in train\n",
      "  File \"c:\\Users\\Alhus\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\tensor.py\", line 221, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"c:\\Users\\Alhus\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 130, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 8.00 GiB total capacity; 6.86 GiB already allocated; 0 bytes free; 7.13 GiB reserved in total by PyTorch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   5 2 0.4 8.0\n",
      "3   5 1 0.2 8.0\n",
      "1   5 1 0.2 8.0\n",
      "2   5 1 0.2 8.0\n",
      "4   5 2 0.6 8.0\n",
      "0   6 2 0.3333333333333333 8.0\n",
      "3   6 1 0.3333333333333333 8.0\n",
      "2   6 1 0.16666666666666666 8.0\n",
      "4   6 2 0.5 8.0\n",
      "1   6 1 0.3333333333333333 8.0\n",
      "3   7 1 0.2857142857142857 8.0\n",
      "0   7 2 0.42857142857142855 8.0\n",
      "4   7 2 0.42857142857142855 8.0\n",
      "2   7 1 0.14285714285714285 8.0\n",
      "1   7 1 0.2857142857142857 8.0\n",
      "3   8 2 0.5 8.0\n",
      "4   8 2 0.375 8.0\n",
      "2   8 1 0.125 8.0\n",
      "0   8 2 0.5 8.0\n",
      "1   8 1 0.25 8.0\n",
      "3   9 2 0.4444444444444444 8.0\n",
      "4   9 2 0.3333333333333333 8.0\n",
      "2   9 1 0.1111111111111111 8.0\n",
      "0   9 2 0.4444444444444444 8.0\n",
      "1   9 1 0.2222222222222222 8.0\n",
      "3   10 2 0.4 8.0\n",
      "3   11 2 0.5454545454545454 8.0\n",
      "4   10 2 0.3 8.0\n",
      "2   10 1 0.1 8.0\n",
      "0   10 2 0.4 8.0\n",
      "1   10 1 0.2 8.0\n",
      "4   11 2 0.2727272727272727 8.0\n",
      "3   12 2 0.5 8.0\n",
      "2   11 1 0.09090909090909091 8.0\n",
      "0   11 2 0.45454545454545453 8.0\n",
      "1   11 1 0.18181818181818182 8.0\n",
      "3   13 2 0.46153846153846156 8.0\n",
      "4   12 2 0.3333333333333333 8.0\n",
      "1   12 1 0.16666666666666666 8.0\n",
      "0   12 2 0.5 8.0\n",
      "2   12 1 0.16666666666666666 8.0\n",
      "3   14 2 0.42857142857142855 8.0\n",
      "0   13 2 0.46153846153846156 8.0\n",
      "1   13 1 0.15384615384615385 8.0\n",
      "4   13 2 0.38461538461538464 8.0\n",
      "2   13 1 0.23076923076923078 8.0\n",
      "3   15 2 0.4 8.0\n",
      "1   14 1 0.21428571428571427 8.0\n",
      "0   14 2 0.42857142857142855 8.0\n",
      "4   14 2 0.35714285714285715 8.0\n",
      "2   14 1 0.21428571428571427 8.0\n",
      "3   16 2 0.4375 8.0\n",
      "1   15 1 0.2 8.0\n",
      "0   15 2 0.4666666666666667 8.0\n",
      "4   15 2 0.3333333333333333 8.0\n",
      "2   15 1 0.2 8.0\n",
      "1   16 1 0.1875 8.0\n",
      "0   16 2 0.4375 8.0\n",
      "3   17 2 0.4117647058823529 8.0\n",
      "4   16 2 0.3125 8.0\n",
      "2   16 1 0.1875 8.0\n",
      "0   17 2 0.4117647058823529 8.0\n",
      "1   17 1 0.17647058823529413 8.0\n",
      "3   18 2 0.3888888888888889 8.0\n",
      "4   17 2 0.29411764705882354 8.0\n",
      "2   17 1 0.17647058823529413 8.0\n",
      "0   18 2 0.3888888888888889 8.0\n",
      "1   18 1 0.16666666666666666 8.0\n",
      "3   19 2 0.3684210526315789 8.0\n",
      "4   18 2 0.2777777777777778 8.0\n",
      "2   18 1 0.16666666666666666 8.0\n",
      "3   20 2 0.35 8.0\n",
      "1   19 1 0.15789473684210525 8.0\n",
      "0   19 2 0.3684210526315789 8.0\n",
      "4   19 2 0.2631578947368421 8.0\n",
      "2   19 1 0.15789473684210525 8.0\n",
      "1   20 1 0.15 8.0\n",
      "0   20 2 0.35 8.0\n",
      "3   21 2 0.3333333333333333 8.0\n",
      "4   20 2 0.25 8.0\n",
      "2   20 1 0.15 8.0\n",
      "0   21 2 0.3333333333333333 8.0\n",
      "1   21 1 0.14285714285714285 8.0\n",
      "3   22 2 0.3181818181818182 8.0\n",
      "4   21 2 0.23809523809523808 8.0\n",
      "2   21 1 0.14285714285714285 8.0\n",
      "0   22 2 0.3181818181818182 8.0\n",
      "3   23 2 0.30434782608695654 8.0\n",
      "1   22 1 0.13636363636363635 8.0\n",
      "4   22 2 0.2727272727272727 8.0\n",
      "2   22 1 0.13636363636363635 8.0\n",
      "0   23 2 0.30434782608695654 8.0\n",
      "1   23 1 0.13043478260869565 8.0\n",
      "3   24 2 0.2916666666666667 8.0\n",
      "4   23 2 0.2608695652173913 8.0\n",
      "0   24 2 0.2916666666666667 8.0\n",
      "2   23 1 0.13043478260869565 8.0\n",
      "1   24 1 0.125 8.0\n",
      "3   25 2 0.28 8.0\n",
      "4   24 2 0.25 8.0\n",
      "0   25 2 0.28 8.0\n",
      "2   24 1 0.125 8.0\n",
      "1   25 1 0.12 8.0\n",
      "3   26 2 0.2692307692307693 8.0\n",
      "4   25 2 0.24 8.0\n",
      "0   26 2 0.2692307692307693 8.0\n",
      "2   25 1 0.12 8.0\n",
      "1   26 1 0.11538461538461539 8.0\n",
      "3   27 2 0.2592592592592593 8.0\n",
      "4   26 2 0.23076923076923078 8.0\n",
      "2   26 1 0.11538461538461539 8.0\n",
      "0   27 2 0.2592592592592593 8.0\n",
      "1   27 1 0.1111111111111111 8.0\n",
      "3   28 2 0.25000000000000006 8.0\n",
      "4   27 2 0.2222222222222222 8.0\n",
      "0   28 2 0.25000000000000006 8.0\n",
      "2   27 1 0.1111111111111111 8.0\n",
      "1   28 1 0.10714285714285714 8.0\n",
      "3   29 2 0.24137931034482765 8.0\n",
      "4   28 2 0.21428571428571427 8.0\n",
      "2   28 1 0.10714285714285714 8.0\n",
      "0   29 2 0.24137931034482765 8.0\n",
      "1   29 1 0.10344827586206896 8.0\n",
      "4   29 2 0.20689655172413793 8.0\n",
      "3   30 2 0.2333333333333334 8.0\n",
      "2   29 1 0.10344827586206896 8.0\n",
      "0   30 2 0.2333333333333334 8.0\n",
      "1   30 1 0.1 8.0\n",
      "4   30 2 0.2 8.0\n",
      "3   31 2 0.22580645161290328 8.0\n",
      "2   30 1 0.1 8.0\n",
      "0   31 2 0.2580645161290323 8.0\n",
      "1   31 1 0.0967741935483871 8.0\n",
      "4   31 2 0.1935483870967742 8.0\n",
      "3   32 2 0.21875000000000006 8.0\n",
      "2   31 1 0.0967741935483871 8.0\n",
      "0   32 2 0.25000000000000006 8.0\n",
      "1   32 1 0.09375 8.0\n",
      "4   32 2 0.1875 8.0\n",
      "3   33 2 0.21212121212121218 8.0\n",
      "2   32 1 0.09375 8.0\n",
      "0   33 2 0.2424242424242425 8.0\n",
      "1   33 1 0.09090909090909091 8.0\n",
      "4   33 2 0.18181818181818182 8.0\n",
      "3   34 2 0.20588235294117652 8.0\n",
      "2   33 1 0.09090909090909091 8.0\n",
      "0   34 2 0.23529411764705888 8.0\n",
      "1   34 1 0.08823529411764706 8.0\n",
      "4   34 2 0.17647058823529413 8.0\n",
      "3   35 2 0.20000000000000004 8.0\n",
      "2   34 1 0.08823529411764706 8.0\n",
      "0   35 2 0.22857142857142862 8.0\n",
      "1   35 1 0.08571428571428572 8.0\n",
      "4   35 2 0.17142857142857143 8.0\n",
      "3   36 2 0.1944444444444445 8.0\n",
      "2   35 1 0.08571428571428572 8.0\n",
      "0   36 2 0.22222222222222227 8.0\n",
      "1   36 1 0.08333333333333333 8.0\n",
      "4   36 2 0.16666666666666666 8.0\n",
      "3   37 2 0.18918918918918923 8.0\n",
      "2   36 1 0.08333333333333333 8.0\n",
      "0   37 2 0.21621621621621626 8.0\n",
      "4   37 2 0.16216216216216217 8.0\n",
      "3   38 2 0.18421052631578952 8.0\n",
      "2   37 1 0.08108108108108109 8.0\n",
      "0   38 2 0.21052631578947373 8.0\n",
      "1   37 1 0.10810810810810811 8.0\n",
      "1   38 2 0.15789473684210525 8.0\n",
      "4   38 2 0.15789473684210525 8.0\n",
      "3   39 2 0.17948717948717954 8.0\n",
      "2   38 1 0.07894736842105263 8.0\n",
      "0   39 2 0.20512820512820518 8.0\n",
      "1   39 2 0.15384615384615385 8.0\n",
      "4   39 2 0.15384615384615385 8.0\n",
      "3   40 2 0.17500000000000004 8.0\n",
      "2   39 1 0.07692307692307693 8.0\n",
      "0   40 2 0.20000000000000004 8.0\n",
      "1   40 2 0.15 8.0\n",
      "4   40 2 0.15 8.0\n",
      "3   41 2 0.1707317073170732 8.0\n",
      "2   40 1 0.075 8.0\n",
      "0   41 2 0.19512195121951223 8.0\n",
      "1   41 2 0.17073170731707318 8.0\n",
      "3   42 2 0.19047619047619052 8.0\n",
      "4   41 2 0.17073170731707318 8.0\n",
      "0   42 2 0.19047619047619052 8.0\n",
      "1   42 2 0.16666666666666669 8.0\n",
      "2   41 1 0.0975609756097561 8.0\n",
      "3   43 2 0.18604651162790703 8.0\n",
      "0   43 2 0.18604651162790703 8.0\n",
      "4   42 2 0.16666666666666669 8.0\n",
      "1   43 2 0.16279069767441862 8.0\n",
      "2   42 1 0.09523809523809523 8.0\n",
      "3   44 2 0.18181818181818185 8.0\n",
      "4   43 2 0.16279069767441862 8.0\n",
      "1   44 2 0.15909090909090912 8.0\n",
      "2   43 1 0.09302325581395349 8.0\n",
      "3   45 2 0.1777777777777778 8.0\n",
      "0   44 2 0.20454545454545459 8.0\n",
      "4   44 2 0.15909090909090912 8.0\n",
      "1   45 2 0.17777777777777778 8.0\n",
      "2   44 1 0.09090909090909091 8.0\n",
      "3   46 2 0.19565217391304351 8.0\n",
      "0   45 2 0.20000000000000004 8.0\n",
      "4   45 2 0.15555555555555559 8.0\n",
      "2   45 1 0.08888888888888889 8.0\n",
      "1   46 2 0.17391304347826086 8.0\n",
      "3   47 2 0.1914893617021277 8.0\n",
      "0   46 2 0.19565217391304351 8.0\n",
      "4   46 2 0.19565217391304351 8.0\n",
      "2   46 1 0.08695652173913043 8.0\n",
      "1   47 2 0.1702127659574468 8.0\n",
      "3   48 2 0.18750000000000003 8.0\n",
      "0   47 2 0.1914893617021277 8.0\n",
      "4   47 2 0.1914893617021277 8.0\n",
      "2   47 1 0.0851063829787234 8.0\n",
      "1   48 2 0.16666666666666666 8.0\n",
      "0   48 2 0.18750000000000003 8.0\n",
      "3   49 2 0.18367346938775514 8.0\n",
      "4   48 2 0.18750000000000003 8.0\n",
      "2   48 1 0.08333333333333333 8.0\n",
      "1   49 2 0.16326530612244897 8.0\n",
      "3   50 2 0.18000000000000005 8.0\n",
      "0   49 2 0.18367346938775514 8.0\n",
      "4   49 2 0.18367346938775514 8.0\n",
      "2   49 1 0.08163265306122448 8.0\n",
      "1   50 2 0.15999999999999998 8.0\n",
      "0   50 2 0.18000000000000005 8.0\n",
      "3   51 2 0.19607843137254904 8.0\n",
      "4   50 2 0.18000000000000005 8.0\n",
      "2   50 1 0.07999999999999999 8.0\n",
      "1   51 2 0.15686274509803919 8.0\n",
      "0   51 2 0.17647058823529416 8.0\n",
      "4   51 2 0.17647058823529416 8.0\n",
      "3   52 2 0.19230769230769235 8.0\n",
      "2   51 1 0.07843137254901959 8.0\n",
      "1   52 2 0.1538461538461538 8.0\n",
      "4   52 2 0.1730769230769231 8.0\n",
      "0   52 2 0.1730769230769231 8.0\n",
      "3   53 2 0.1886792452830189 8.0\n",
      "2   52 1 0.0769230769230769 8.0\n",
      "1   53 2 0.15094339622641503 8.0\n",
      "4   53 2 0.16981132075471703 8.0\n",
      "0   53 2 0.16981132075471703 8.0\n",
      "3   54 2 0.18518518518518523 8.0\n",
      "2   53 1 0.07547169811320752 8.0\n",
      "1   54 2 0.14814814814814808 8.0\n",
      "4   54 2 0.1666666666666667 8.0\n",
      "0   54 2 0.1666666666666667 8.0\n",
      "3   55 2 0.18181818181818185 8.0\n",
      "2   54 1 0.07407407407407404 8.0\n",
      "1   55 2 0.1454545454545454 8.0\n",
      "4   55 2 0.16363636363636366 8.0\n",
      "0   55 2 0.16363636363636366 8.0\n",
      "3   56 2 0.1785714285714286 8.0\n",
      "2   55 1 0.0727272727272727 8.0\n",
      "1   56 2 0.1428571428571428 8.0\n",
      "4   56 2 0.16071428571428575 8.0\n",
      "0   56 2 0.16071428571428575 8.0\n",
      "3   57 2 0.1754385964912281 8.0\n",
      "2   56 1 0.0714285714285714 8.0\n",
      "1   57 2 0.1403508771929824 8.0\n",
      "4   57 2 0.15789473684210528 8.0\n",
      "0   57 2 0.15789473684210528 8.0\n",
      "3   58 2 0.17241379310344832 8.0\n",
      "2   57 1 0.0701754385964912 8.0\n",
      "1   58 2 0.13793103448275856 8.0\n",
      "4   58 2 0.15517241379310348 8.0\n",
      "0   58 2 0.15517241379310348 8.0\n",
      "3   59 2 0.16949152542372883 8.0\n",
      "2   58 1 0.06896551724137928 8.0\n",
      "1   59 2 0.135593220338983 8.0\n",
      "4   59 2 0.15254237288135597 8.0\n",
      "0   59 2 0.15254237288135597 8.0\n",
      "3   60 2 0.16666666666666669 8.0\n",
      "2   59 1 0.0677966101694915 8.0\n",
      "1   60 2 0.13333333333333328 8.0\n",
      "4   60 2 0.15000000000000002 8.0\n",
      "0   60 2 0.15000000000000002 8.0\n",
      "3   61 2 0.1639344262295082 8.0\n",
      "2   60 1 0.06666666666666664 8.0\n",
      "1   61 2 0.1311475409836065 8.0\n",
      "4   61 2 0.1475409836065574 8.0\n",
      "0   61 2 0.1475409836065574 8.0\n",
      "3   62 2 0.16129032258064518 8.0\n",
      "2   61 1 0.06557377049180325 8.0\n",
      "1   62 2 0.1290322580645161 8.0\n",
      "4   62 2 0.14516129032258068 8.0\n",
      "0   62 2 0.14516129032258068 8.0\n",
      "3   63 2 0.15873015873015875 8.0\n",
      "2   62 1 0.06451612903225805 8.0\n",
      "1   63 2 0.12698412698412695 8.0\n",
      "4   63 2 0.14285714285714288 8.0\n",
      "0   63 2 0.14285714285714288 8.0\n",
      "3   64 2 0.15625000000000003 8.0\n",
      "2   63 1 0.06349206349206347 8.0\n",
      "1   64 2 0.12499999999999997 8.0\n",
      "4   64 2 0.14062500000000003 8.0\n",
      "0   64 2 0.14062500000000003 8.0\n",
      "3   65 2 0.15384615384615388 8.0\n",
      "2   64 1 0.062499999999999986 8.0\n",
      "1   65 2 0.12307692307692306 8.0\n",
      "4   65 2 0.1384615384615385 8.0\n",
      "0   65 2 0.1384615384615385 8.0\n",
      "3   66 2 0.15151515151515155 8.0\n",
      "2   65 1 0.06153846153846153 8.0\n",
      "1   66 2 0.12121212121212119 8.0\n",
      "4   66 2 0.13636363636363638 8.0\n",
      "0   66 2 0.13636363636363638 8.0\n",
      "3   67 2 0.1492537313432836 8.0\n",
      "2   66 1 0.060606060606060594 8.0\n",
      "1   67 2 0.11940298507462684 8.0\n",
      "4   67 2 0.13432835820895525 8.0\n",
      "0   67 2 0.13432835820895525 8.0\n",
      "3   68 2 0.1470588235294118 8.0\n",
      "2   67 1 0.05970149253731342 8.0\n",
      "1   68 2 0.11764705882352938 8.0\n",
      "4   68 2 0.13235294117647062 8.0\n",
      "0   68 2 0.13235294117647062 8.0\n",
      "3   69 2 0.1449275362318841 8.0\n",
      "1   69 2 0.11594202898550722 8.0\n",
      "2   68 1 0.05882352941176469 8.0\n",
      "4   69 2 0.13043478260869568 8.0\n",
      "0   69 2 0.13043478260869568 8.0\n",
      "3   70 2 0.14285714285714288 8.0\n",
      "2   69 1 0.05797101449275361 8.0\n",
      "1   70 2 0.11428571428571425 8.0\n",
      "4   70 2 0.1285714285714286 8.0\n",
      "0   70 2 0.1285714285714286 8.0\n",
      "3   71 2 0.14084507042253525 8.0\n",
      "2   70 1 0.05714285714285713 8.0\n",
      "1   71 2 0.11267605633802814 8.0\n",
      "4   71 2 0.12676056338028172 8.0\n",
      "0   71 2 0.12676056338028172 8.0\n",
      "3   72 2 0.13888888888888892 8.0\n",
      "2   71 1 0.05633802816901407 8.0\n",
      "1   72 2 0.11111111111111109 8.0\n",
      "4   72 2 0.12500000000000003 8.0\n",
      "0   72 2 0.12500000000000003 8.0\n",
      "3   73 2 0.13698630136986303 8.0\n",
      "2   72 1 0.055555555555555546 8.0\n",
      "1   73 2 0.10958904109589039 8.0\n",
      "4   73 2 0.12328767123287673 8.0\n",
      "0   73 2 0.12328767123287673 8.0\n",
      "3   74 2 0.13513513513513517 8.0\n",
      "2   73 1 0.054794520547945195 8.0\n",
      "1   74 2 0.10810810810810809 8.0\n",
      "4   74 2 0.12162162162162164 8.0\n",
      "0   74 2 0.12162162162162164 8.0\n",
      "3   75 2 0.13333333333333336 8.0\n",
      "2   74 1 0.05405405405405404 8.0\n",
      "1   75 2 0.10666666666666665 8.0\n",
      "4   75 2 0.12000000000000002 8.0\n",
      "0   75 2 0.12000000000000002 8.0\n",
      "3   76 2 0.13157894736842107 8.0\n",
      "1   76 2 0.10526315789473682 8.0\n",
      "2   75 1 0.05333333333333332 8.0\n",
      "4   76 2 0.11842105263157897 8.0\n",
      "0   76 2 0.11842105263157897 8.0\n",
      "3   77 2 0.12987012987012989 8.0\n",
      "1   77 2 0.10389610389610388 8.0\n",
      "2   76 1 0.05263157894736841 8.0\n",
      "4   77 2 0.11688311688311691 8.0\n",
      "0   77 2 0.11688311688311691 8.0\n",
      "1   78 2 0.10256410256410255 8.0\n",
      "3   78 2 0.12820512820512822 8.0\n",
      "2   77 1 0.05194805194805194 8.0\n",
      "4   78 2 0.1153846153846154 8.0\n",
      "2   78 1 0.05128205128205127 8.0\n",
      "1   79 2 0.1012658227848101 8.0\n",
      "3   79 2 0.1265822784810127 8.0\n",
      "0   78 2 0.1153846153846154 8.0\n",
      "4   79 2 0.11392405063291142 8.0\n",
      "3   80 2 0.12500000000000006 8.0\n",
      "2   79 1 0.05063291139240505 8.0\n",
      "1   80 2 0.09999999999999996 8.0\n",
      "0   79 2 0.11392405063291142 8.0\n",
      "4   80 2 0.11250000000000002 8.0\n",
      "3   81 2 0.12345679012345684 8.0\n",
      "1   81 2 0.0987654320987654 8.0\n",
      "2   80 1 0.04999999999999998 8.0\n",
      "0   80 2 0.11250000000000002 8.0\n",
      "4   81 2 0.11111111111111113 8.0\n",
      "1   82 2 0.09756097560975606 8.0\n",
      "2   81 1 0.0493827160493827 8.0\n",
      "3   82 2 0.12195121951219516 8.0\n",
      "0   81 2 0.11111111111111113 8.0\n",
      "4   82 2 0.10975609756097564 8.0\n",
      "1   83 2 0.09638554216867466 8.0\n",
      "2   82 1 0.04878048780487803 8.0\n",
      "3   83 2 0.12048192771084341 8.0\n",
      "0   82 2 0.10975609756097564 8.0\n",
      "4   83 2 0.10843373493975905 8.0\n",
      "1   84 2 0.0952380952380952 8.0\n",
      "2   83 1 0.04819277108433733 8.0\n",
      "3   84 2 0.1190476190476191 8.0\n",
      "0   83 2 0.10843373493975905 8.0\n",
      "4   84 2 0.10714285714285716 8.0\n",
      "1   85 2 0.0941176470588235 8.0\n",
      "2   84 1 0.0476190476190476 8.0\n",
      "0   84 2 0.10714285714285716 8.0\n",
      "3   85 2 0.11764705882352945 8.0\n",
      "4   85 2 0.1058823529411765 8.0\n",
      "1   86 2 0.09302325581395346 8.0\n",
      "2   85 1 0.04705882352941175 8.0\n",
      "0   85 2 0.1058823529411765 8.0\n",
      "3   86 2 0.1162790697674419 8.0\n",
      "4   86 2 0.1046511627906977 8.0\n",
      "1   87 2 0.09195402298850572 8.0\n",
      "2   86 1 0.04651162790697673 8.0\n",
      "3   87 2 0.11494252873563222 8.0\n",
      "0   86 2 0.1046511627906977 8.0\n",
      "4   87 2 0.10344827586206899 8.0\n",
      "1   88 2 0.09090909090909088 8.0\n",
      "2   87 1 0.04597701149425286 8.0\n",
      "3   88 2 0.11363636363636367 8.0\n",
      "4   88 2 0.10227272727272729 8.0\n",
      "0   87 2 0.10344827586206899 8.0\n",
      "1   89 2 0.08988764044943819 8.0\n",
      "2   88 1 0.04545454545454544 8.0\n",
      "3   89 2 0.11235955056179779 8.0\n",
      "4   89 2 0.101123595505618 8.0\n",
      "0   88 2 0.10227272727272729 8.0\n",
      "1   90 2 0.08888888888888886 8.0\n",
      "3   90 2 0.11111111111111115 8.0\n",
      "2   89 1 0.04494382022471909 8.0\n",
      "4   90 2 0.10000000000000002 8.0\n",
      "0   89 2 0.101123595505618 8.0\n",
      "1   91 2 0.08791208791208789 8.0\n",
      "3   91 2 0.10989010989010993 8.0\n",
      "2   90 1 0.04444444444444443 8.0\n",
      "4   91 2 0.09890109890109892 8.0\n",
      "0   90 2 0.10000000000000002 8.0\n",
      "1   92 2 0.08695652173913042 8.0\n",
      "2   91 1 0.043956043956043946 8.0\n",
      "34   92 2 0.09782608695652176 8.0\n",
      "   92 2 0.10869565217391308 8.0\n",
      "0   91 2 0.09890109890109892 8.0\n",
      "1   93 2 0.08602150537634407 8.0\n",
      "2   92 1 0.04347826086956521 8.0\n",
      "4   93 2 0.09677419354838712 8.0\n",
      "3   93 2 0.10752688172043015 8.0\n",
      "0   92 2 0.09782608695652176 8.0\n",
      "1   94 2 0.08510638297872339 8.0\n",
      "4   94 2 0.09574468085106384 8.0\n",
      "2   93 1 0.04301075268817203 8.0\n",
      "3   94 2 0.1063829787234043 8.0\n",
      "0   93 2 0.09677419354838712 8.0\n",
      "4   95 2 0.09473684210526317 8.0\n",
      "1   95 2 0.08421052631578946 8.0\n",
      "3   95 2 0.10526315789473688 8.0\n",
      "2   94 1 0.042553191489361694 8.0\n",
      "0   94 2 0.09574468085106384 8.0\n",
      "4   96 2 0.09375000000000001 8.0\n",
      "3   96 2 0.1041666666666667 8.0\n",
      "1   96 2 0.08333333333333331 8.0\n",
      "2   95 1 0.04210526315789473 8.0\n",
      "0   95 2 0.09473684210526317 8.0\n",
      "4   97 2 0.09278350515463919 8.0\n",
      "3   97 2 0.10309278350515468 8.0\n",
      "1   97 2 0.0824742268041237 8.0\n",
      "2   96 1 0.04166666666666666 8.0\n",
      "0   96 2 0.09375000000000001 8.0\n",
      "4   98 2 0.09183673469387757 8.0\n",
      "3   98 2 0.10204081632653064 8.0\n",
      "2   97 1 0.04123711340206185 8.0\n",
      "1   98 2 0.08163265306122447 8.0\n",
      "0   97 2 0.09278350515463919 8.0\n",
      "4   99 2 0.09090909090909093 8.0\n",
      "3   99 2 0.10101010101010105 8.0\n",
      "1   99 2 0.08080808080808079 8.0\n",
      "2   98 1 0.040816326530612235 8.0\n",
      "0   98 2 0.09183673469387757 8.0\n",
      "2   99 1 0.040404040404040394 8.0\n",
      "0   99 2 0.09090909090909093 8.0\n",
      "0.8464817248906141\n",
      "1   0 0 0 8.0\n",
      "3   0 0 0 8.0\n",
      "2   0 0 0 8.0\n",
      "0   0 0 0 8.0\n",
      "5   0 0 0 8.0\n",
      "4   0 0 0 8.0\n",
      "5   1 0 0.0 8.0\n",
      "3   1 0 0.0 8.0\n",
      "1   1 0 0.0 8.0\n",
      "2   1 0 0.0 8.0\n",
      "4   1 1 1.0 8.0\n",
      "0   1 2 2.0 8.0\n",
      "2   2 0 0.0 8.0\n",
      "0   2 2 1.0 8.0\n",
      "1   2 1 0.5 8.0\n",
      "5   2 0 0.0 8.0\n",
      "4   2 1 1.0 8.0\n",
      "3   2 1 0.5 8.0\n",
      "2   3 0 0.0 8.0\n",
      "0   3 2 0.6666666666666666 8.0\n",
      "1   3 1 0.3333333333333333 8.0\n",
      "5   3 1 0.3333333333333333 8.0\n",
      "4   3 1 1.0 8.0\n",
      "3   3 1 0.6666666666666666 8.0\n",
      "0   4 2 0.5 8.0\n",
      "2   4 0 0.0 8.0\n",
      "5   4 1 0.25 8.0\n",
      "1   4 1 0.5 8.0\n",
      "3   4 1 0.5 8.0\n",
      "4   4 1 0.75 8.0\n",
      "2   5 0 0.0 8.0\n",
      "5   5 1 0.2 8.0\n",
      "1   5 1 0.4 8.0\n",
      "3   5 1 0.4 8.0\n",
      "4   5 1 0.6 8.0\n",
      "0   5 2 0.6 8.0\n",
      "4   6 2 0.8333333333333334 8.0\n",
      "2   6 0 0.0 8.0\n",
      "4   7 2 0.7142857142857143 8.0\n",
      "5   6 2 0.5 8.0\n",
      "1   6 1 0.5 8.0\n",
      "3   6 1 0.5 8.0\n",
      "0   6 3 1.0 8.0\n",
      "2   7 0 0.0 8.0\n",
      "5   7 2 0.42857142857142855 8.0\n",
      "1   7 1 0.42857142857142855 8.0\n",
      "4   8 2 0.625 8.0\n",
      "3   7 2 0.7142857142857143 8.0\n",
      "0   7 3 1.0 8.0\n",
      "5   8 2 0.375 8.0\n",
      "2   8 1 0.125 8.0\n",
      "4   9 2 0.5555555555555556 8.0\n",
      "3   8 2 0.625 8.0\n",
      "1   8 1 0.5 8.0\n",
      "0   8 3 0.875 8.0\n",
      "5   9 2 0.3333333333333333 8.0\n",
      "2   9 1 0.1111111111111111 8.0\n",
      "3   9 2 0.5555555555555556 8.0\n",
      "4   10 2 0.5 8.0\n",
      "1   9 1 0.4444444444444444 8.0\n",
      "5   10 2 0.3 8.0\n",
      "0   9 3 0.7777777777777778 8.0\n",
      "3   10 2 0.5 8.0\n",
      "2   10 1 0.2 8.0\n",
      "4   11 2 0.45454545454545453 8.0\n",
      "5   11 2 0.2727272727272727 8.0\n",
      "3   11 2 0.45454545454545453 8.0\n",
      "2   11 1 0.18181818181818182 8.0\n",
      "1   10 2 0.6 8.0\n",
      "0   10 3 0.8 8.0\n",
      "5   12 2 0.25 8.0\n",
      "4   12 2 0.5 8.0\n",
      "3   12 2 0.4166666666666667 8.0\n",
      "2   12 1 0.16666666666666666 8.0\n",
      "1   11 2 0.5454545454545454 8.0\n",
      "0   11 3 0.7272727272727273 8.0\n",
      "5   13 2 0.23076923076923078 8.0\n",
      "4   13 2 0.46153846153846156 8.0\n",
      "3   13 2 0.38461538461538464 8.0\n",
      "1   12 2 0.5 8.0\n",
      "2   13 1 0.23076923076923078 8.0\n",
      "5   14 2 0.2857142857142857 8.0\n",
      "0   12 3 0.75 8.0\n",
      "4   14 2 0.5 8.0\n",
      "1   13 2 0.46153846153846156 8.0\n",
      "2   14 1 0.21428571428571427 8.0\n",
      "5   15 2 0.26666666666666666 8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Alhus\\anaconda3\\envs\\pytorch\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\Alhus\\anaconda3\\envs\\pytorch\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Alhus\\AppData\\Local\\Temp\\ipykernel_6408\\3210863791.py\", line 31, in run_new_model\n",
      "  File \"C:\\Users\\Alhus\\AppData\\Local\\Temp\\ipykernel_6408\\2193369100.py\", line 110, in train\n",
      "  File \"c:\\Users\\Alhus\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\grad_mode.py\", line 26, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Alhus\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\adam.py\", line 108, in step\n",
      "    F.adam(params_with_grad,\n",
      "  File \"c:\\Users\\Alhus\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\functional.py\", line 94, in adam\n",
      "    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 8.00 GiB total capacity; 6.81 GiB already allocated; 0 bytes free; 7.13 GiB reserved in total by PyTorch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3   14 2 0.42857142857142855 8.0\n",
      "4   15 2 0.4666666666666667 8.0\n",
      "1   14 2 0.42857142857142855 8.0\n",
      "2   15 1 0.2 8.0\n",
      "5   16 2 0.25 8.0\n",
      "3   15 2 0.4 8.0\n",
      "4   16 2 0.4375 8.0\n",
      "1   15 2 0.4 8.0\n",
      "2   16 1 0.1875 8.0\n",
      "5   17 2 0.23529411764705882 8.0\n",
      "3   16 2 0.375 8.0\n",
      "1   16 2 0.375 8.0\n",
      "4   17 2 0.4117647058823529 8.0\n",
      "2   17 1 0.17647058823529413 8.0\n",
      "5   18 2 0.2222222222222222 8.0\n",
      "3   17 2 0.35294117647058826 8.0\n",
      "4   18 2 0.3888888888888889 8.0\n",
      "1   17 2 0.35294117647058826 8.0\n",
      "2   18 1 0.16666666666666666 8.0\n",
      "5   19 2 0.21052631578947367 8.0\n",
      "3   18 2 0.3333333333333333 8.0\n",
      "1   18 2 0.3333333333333333 8.0\n",
      "4   19 2 0.42105263157894735 8.0\n",
      "2   19 1 0.15789473684210525 8.0\n",
      "5   20 2 0.2 8.0\n",
      "3   19 2 0.3157894736842105 8.0\n",
      "1   19 2 0.3157894736842105 8.0\n",
      "4   20 2 0.4 8.0\n",
      "2   20 1 0.15 8.0\n",
      "5   21 2 0.19047619047619047 8.0\n",
      "3   20 2 0.35 8.0\n",
      "1   20 2 0.3 8.0\n",
      "4   21 2 0.38095238095238093 8.0\n",
      "2   21 1 0.14285714285714285 8.0\n",
      "5   22 2 0.18181818181818182 8.0\n",
      "3   21 2 0.3333333333333333 8.0\n",
      "1   21 2 0.2857142857142857 8.0\n",
      "4   22 2 0.36363636363636365 8.0\n",
      "2   22 1 0.13636363636363635 8.0\n",
      "5   23 2 0.17391304347826086 8.0\n",
      "3   22 2 0.3181818181818182 8.0\n",
      "1   22 2 0.2727272727272727 8.0\n",
      "4   23 2 0.34782608695652173 8.0\n",
      "2   23 1 0.13043478260869565 8.0\n",
      "5   24 2 0.16666666666666666 8.0\n",
      "3   23 2 0.30434782608695654 8.0\n",
      "1   23 2 0.2608695652173913 8.0\n",
      "4   24 2 0.375 8.0\n",
      "2   24 1 0.125 8.0\n",
      "5   25 2 0.16 8.0\n",
      "3   24 2 0.2916666666666667 8.0\n",
      "1   24 2 0.25 8.0\n",
      "4   25 2 0.36 8.0\n",
      "2   25 1 0.12 8.0\n",
      "3   25 2 0.28 8.0\n",
      "5   26 2 0.15384615384615385 8.0\n",
      "1   25 2 0.24 8.0\n",
      "4   26 2 0.34615384615384615 8.0\n",
      "2   26 1 0.11538461538461539 8.0\n",
      "3   26 2 0.3076923076923077 8.0\n",
      "5   27 2 0.14814814814814814 8.0\n",
      "1   26 2 0.23076923076923078 8.0\n",
      "4   27 2 0.3333333333333333 8.0\n",
      "2   27 1 0.1111111111111111 8.0\n",
      "3   27 2 0.2962962962962963 8.0\n",
      "5   28 2 0.14285714285714285 8.0\n",
      "1   27 2 0.2222222222222222 8.0\n",
      "4   28 2 0.32142857142857145 8.0\n",
      "2   28 1 0.10714285714285714 8.0\n",
      "3   28 2 0.2857142857142857 8.0\n",
      "5   29 2 0.13793103448275862 8.0\n",
      "1   28 2 0.21428571428571427 8.0\n",
      "4   29 2 0.3448275862068966 8.0\n",
      "2   29 1 0.10344827586206896 8.0\n",
      "3   29 2 0.27586206896551724 8.0\n",
      "5   30 2 0.13333333333333333 8.0\n",
      "1   29 2 0.20689655172413793 8.0\n",
      "2   30 1 0.1 8.0\n",
      "4   30 2 0.3333333333333333 8.0\n",
      "3   30 2 0.26666666666666666 8.0\n",
      "5   31 2 0.12903225806451613 8.0\n",
      "1   30 2 0.2 8.0\n",
      "2   31 1 0.0967741935483871 8.0\n",
      "4   31 2 0.3548387096774194 8.0\n",
      "3   31 2 0.25806451612903225 8.0\n",
      "5   32 2 0.125 8.0\n",
      "1   31 2 0.1935483870967742 8.0\n",
      "2   32 1 0.09375 8.0\n",
      "4   32 2 0.34375 8.0\n",
      "3   32 2 0.25 8.0\n",
      "5   33 2 0.12121212121212122 8.0\n",
      "1   32 2 0.1875 8.0\n",
      "2   33 1 0.09090909090909091 8.0\n",
      "4   33 2 0.3333333333333333 8.0\n",
      "3   33 2 0.24242424242424243 8.0\n",
      "1   33 2 0.18181818181818182 8.0\n",
      "5   34 2 0.14705882352941177 8.0\n",
      "4   34 2 0.3235294117647059 8.0\n",
      "2   34 1 0.08823529411764706 8.0\n",
      "3   34 2 0.2647058823529412 8.0\n",
      "1   34 2 0.17647058823529413 8.0\n",
      "4   35 2 0.3142857142857143 8.0\n",
      "2   35 1 0.08571428571428572 8.0\n",
      "5   35 2 0.14285714285714285 8.0\n",
      "3   35 2 0.2571428571428571 8.0\n",
      "5   36 2 0.1388888888888889 8.0\n",
      "1   35 2 0.17142857142857143 8.0\n",
      "4   36 2 0.3333333333333333 8.0\n",
      "2   36 1 0.08333333333333333 8.0\n",
      "3   36 2 0.25 8.0\n",
      "5   37 2 0.13513513513513514 8.0\n",
      "2   37 1 0.08108108108108109 8.0\n",
      "4   37 2 0.32432432432432434 8.0\n",
      "1   36 2 0.19444444444444445 8.0\n",
      "3   37 2 0.24324324324324326 8.0\n",
      "5   38 2 0.13157894736842105 8.0\n",
      "2   38 1 0.07894736842105263 8.0\n",
      "4   38 2 0.3157894736842105 8.0\n",
      "1   37 2 0.1891891891891892 8.0\n",
      "5   39 2 0.1282051282051282 8.0\n",
      "3   38 2 0.23684210526315788 8.0\n",
      "4   39 2 0.3076923076923077 8.0\n",
      "2   39 1 0.07692307692307693 8.0\n",
      "3   39 2 0.23076923076923078 8.0\n",
      "1   38 2 0.18421052631578946 8.0\n",
      "5   40 2 0.125 8.0\n",
      "4   40 2 0.3 8.0\n",
      "2   40 1 0.075 8.0\n",
      "1   39 2 0.1794871794871795 8.0\n",
      "3   40 2 0.225 8.0\n",
      "5   41 2 0.12195121951219512 8.0\n",
      "2   41 1 0.07317073170731707 8.0\n",
      "4   41 2 0.2926829268292683 8.0\n",
      "3   41 2 0.21951219512195122 8.0\n",
      "5   42 2 0.11904761904761904 8.0\n"
     ]
    }
   ],
   "source": [
    "number_of_threads = 6\n",
    "\n",
    "from threading import Thread \n",
    "run = 0 \n",
    "borne = 0.5\n",
    "\n",
    "while(borne>0.05):\n",
    "    borne = np.exp(-run/6)\n",
    "    print(borne)\n",
    "    threads = []\n",
    "    for i in range(number_of_threads):\n",
    "            # Create a thread for this combination of parameter values and start it\n",
    "        c= np.array([np.random.uniform(np.clip(x-borne,0,1),np.clip(x+borne,0,1)) for x in best_c])\n",
    "        t = Thread(target=run_new_model, args=(c, num_episodes,i))\n",
    "        t.start()\n",
    "        \n",
    "        # Add the thread to the list\n",
    "        threads.append(t)\n",
    "    # Wait for all the threads to finish\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    run+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.system('shutdown /s /t 0')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
